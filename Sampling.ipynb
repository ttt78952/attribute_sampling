{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main functions\n",
    "import networkx as nx\n",
    "import community\n",
    "import random as rdm\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import dit\n",
    "import queue \n",
    "from dit.divergences import jensen_shannon_divergence\n",
    "from scipy.stats import entropy\n",
    "from numpy.linalg import norm\n",
    "from scipy.sparse import linalg\n",
    "\n",
    "# Utilities\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "from os import listdir\n",
    "from scipy.io import loadmat\n",
    "from matplotlib2tikz import save as tikz_save\n",
    "from matplotlib2tikz import get_tikz_code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load graph and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement graph as networkX\n",
    "# facebook100 graphs use Matlab matrix \n",
    "# others use txt files\n",
    "def load_graph(path, graph_type):\n",
    "    if graph_type == \"Directed\":\n",
    "        graph = nx.DiGraph()\n",
    "        with open(path) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                edge = line.strip().split()\n",
    "                graph.add_edge(*edge)\n",
    "        return graph\n",
    "    elif graph_type == \"Undirected\":\n",
    "        graph = nx.Graph()\n",
    "        with open(path) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                edge = line.strip().split()\n",
    "                graph.add_edge(*edge)\n",
    "        return graph\n",
    "    elif graph_type == \"facebook100\":\n",
    "        adj_matrix = loadmat(path)['A'].toarray()\n",
    "            # A -- adjacency matrix\n",
    "            # local_info -- attributes\n",
    "        graph = nx.from_numpy_matrix(adj_matrix)\n",
    "        return graph\n",
    "    else:\n",
    "        print(\"graph type not recognized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build attribute list and bucket\n",
    "# facebook100 graphs use Matlab matrix, they have at most 2 attributes for each nodes\n",
    "# citation graph, wiki use one-to-one table of node attributes\n",
    "# other ground-truth communities graphs use buckets without attributes' name\n",
    "def build_attribute_list_and_bucket(path, graph_type):\n",
    "    if graph_type == \"facebook100\":\n",
    "        attribute_matrix = loadmat(path)['local_info']\n",
    "        attribute_list = {}\n",
    "        for i in range(len(attribute_matrix)):\n",
    "            major = attribute_matrix[i][2]\n",
    "            major_2 = attribute_matrix[i][3]\n",
    "            year = attribute_matrix[i][5]       \n",
    "            attribute_list[i] = (major, major_2, year)\n",
    "        \n",
    "        attribute_bucket = {}\n",
    "        for k in attribute_list.keys():\n",
    "            first_pair = (attribute_list[k][0], attribute_list[k][2])\n",
    "            if first_pair in attribute_bucket:\n",
    "                attribute_bucket[first_pair].append(k)\n",
    "            else:\n",
    "                attribute_bucket[first_pair] = [k]\n",
    "            if attribute_list[k][1] != 0:\n",
    "                second_pair = (attribute_list[k][1], attribute_list[k][2])\n",
    "                if second_pair in attribute_bucket:\n",
    "                    attribute_bucket[second_pair].append(k)\n",
    "                else:\n",
    "                    attribute_bucket[second_pair] = [k]\n",
    "        return attribute_list, attribute_bucket\n",
    "\n",
    "    \n",
    "    elif graph_type == \"cit-Patents\":\n",
    "        attribute_list = {}\n",
    "        attribute_bucket = {}\n",
    "        with open(path, 'r') as document:\n",
    "            lines = document.readlines()\n",
    "            for line in lines:\n",
    "                instance = line.strip().split(',')\n",
    "                attribute_list[instance[0]] = [instance[11]]\n",
    "        for k in attribute_list.keys():\n",
    "            if attribute_list[k][0] not in attribute_bucket:\n",
    "                attribute_bucket[attribute_list[k][0]] = [k]\n",
    "            else:\n",
    "                attribute_bucket[attribute_list[k][0]].append(k)\n",
    "        return attribute_list, attribute_bucket\n",
    "    \n",
    "    \n",
    "    elif graph_type == \"communities\":\n",
    "        attribute_list = {}\n",
    "        attribute_bucket = {}\n",
    "        with open(path, 'r') as document:\n",
    "            bucket_index = 0\n",
    "            lines = document.readlines()\n",
    "            for line in lines:\n",
    "                instance = line.strip().split('\\t')\n",
    "                attribute_bucket[bucket_index] = instance\n",
    "                for i in instance:\n",
    "                    if i not in attribute_list.keys():\n",
    "                        attribute_list[i] = [bucket_index]\n",
    "                    else:\n",
    "                        attribute_list[i].append(bucket_index)\n",
    "                bucket_index += 1\n",
    "        return attribute_list, attribute_bucket\n",
    "    \n",
    "    elif graph_type == \"wiki\":\n",
    "        attribute_list = {}\n",
    "        attribute_bucket = {}\n",
    "        with open(path, 'r') as document:\n",
    "            lines = document.readlines()\n",
    "            for line in lines:\n",
    "                instance = line.strip().split(' ')\n",
    "                attribute_bucket[instance[0]] = instance[1:]\n",
    "                for i in instance[1:]:\n",
    "                    if i not in attribute_list.keys():\n",
    "                        attribute_list[i] = [instance[0]]\n",
    "                    else:\n",
    "                        attribute_list[i].append(instance[0])\n",
    "        return attribute_list, attribute_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is same as above but use fake attributes generated by Louvain algorithm\n",
    "def generate_attribute_list_and_bucket(graph):\n",
    "    if graph.is_directed():\n",
    "        partition = community.best_partition(graph.to_undirected())\n",
    "    else:\n",
    "        partition = community.best_partition(graph)\n",
    "    attribute_list = {}\n",
    "    attribute_bucket = {}\n",
    "    for i in partition.keys():\n",
    "        attribute_list[i] = [partition[i]]\n",
    "    for k in attribute_list.keys():\n",
    "        if attribute_list[k][0] not in attribute_bucket:\n",
    "            attribute_bucket[attribute_list[k][0]] = [k]\n",
    "        else:\n",
    "            attribute_bucket[attribute_list[k][0]].append(k)\n",
    "    return attribute_list, attribute_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate alpha based on percentage of nodes sampled from random walk\n",
    "# calculate_alpha(g, ab, 1) = 0\n",
    "def calculate_alpha(graph, attribute_list, attribute_bucket, random_walk_percentage):\n",
    "    number_nodes = nx.number_of_nodes(graph)\n",
    "    number_attributed_nodes = len(attribute_list)\n",
    "    totalcount = 0\n",
    "    for i in attribute_bucket.values():\n",
    "        totalcount += len(i)**2\n",
    "    total_attri = 0\n",
    "    for i in attribute_list:\n",
    "        total_attri+=len(i)\n",
    "    prior_visit_attributed_nodes = number_attributed_nodes/number_nodes\n",
    "    expectation_bucketnodes = totalcount*total_attri/(number_attributed_nodes*number_attributed_nodes)\n",
    "    return ((1/random_walk_percentage)-1)/(prior_visit_attributed_nodes*expectation_bucketnodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random node sampling\n",
    "def random_node_sampling(graph, percentage, seed = None):\n",
    "    # set random seed\n",
    "    rdm.seed(seed)\n",
    "    samplednodes = rdm.sample(graph.nodes(), k = round(nx.number_of_nodes(graph)*percentage))\n",
    "    return graph.subgraph(samplednodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random walk sampling\n",
    "def random_walk_sampling(und_graph, sample_percentage, jump_iteration = 100, seed = None):\n",
    "    # set random seed\n",
    "    rdm.seed(seed)\n",
    "    \n",
    "    # sample size round down to interger\n",
    "    sample_size = int(nx.number_of_nodes(und_graph) * sample_percentage)\n",
    "    \n",
    "    # set starting node\n",
    "    startnode = rdm.sample(und_graph.nodes(),1)[0]\n",
    "    currentnode = startnode\n",
    "    \n",
    "    # used for jump when no new node visited in certain iteration\n",
    "    restart_iteration = 0 \n",
    "    last_number_of_nodes = 0\n",
    "    \n",
    "    # result node set and total iteration\n",
    "    nodelist = set()\n",
    "    # visited_time = {}\n",
    "    total_iteration = 0\n",
    "    \n",
    "    while len(nodelist) < sample_size:\n",
    "        # add current node\n",
    "        total_iteration += 1\n",
    "        '''\n",
    "        degree = und_graph.degree(currentnode)\n",
    "        if degree not in visited_time:\n",
    "            visited_time[degree] = 1\n",
    "        else:\n",
    "            visited_time[degree] += 1\n",
    "        '''\n",
    "        nodelist.add(currentnode)\n",
    "        \n",
    "        # move a step forward\n",
    "        nextnode = rdm.sample(list(und_graph[currentnode]),1)[0]\n",
    "        currentnode = nextnode\n",
    "        \n",
    "        # find a new startnode if number of nodes in sample does not grow\n",
    "        if restart_iteration < jump_iteration:\n",
    "            restart_iteration += 1\n",
    "        else:\n",
    "            if last_number_of_nodes == len(nodelist):\n",
    "                startnode = rdm.sample(und_graph.nodes(),1)[0]\n",
    "                currentnode = startnode\n",
    "            restart_iteration = 0\n",
    "            last_number_of_nodes = len(nodelist)\n",
    "    return und_graph.subgraph(nodelist), total_iteration #, visited_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random walk with restart sampling\n",
    "def random_walk_with_restart_sampling(und_graph, sample_percentage, restart_prob, jump_iteration = 100, seed = None):\n",
    "    # set random seed\n",
    "    rdm.seed(seed)\n",
    "    \n",
    "    # sample size round down to interger\n",
    "    sample_size = int(nx.number_of_nodes(und_graph) * sample_percentage)\n",
    "    \n",
    "    # set starting node\n",
    "    startnode = rdm.sample(und_graph.nodes(),1)[0]\n",
    "    currentnode = startnode\n",
    "    \n",
    "    # used for jump when no new node visited in certain iteration\n",
    "    restart_iteration = 0 \n",
    "    last_number_of_nodes = 0\n",
    "    \n",
    "    # result node set and total iteration\n",
    "    nodelist = set()\n",
    "    total_iteration = 0\n",
    "    \n",
    "    while len(nodelist) < sample_size:\n",
    "        # add current node\n",
    "        total_iteration += 1\n",
    "        \n",
    "        \n",
    "        nodelist.add(currentnode)\n",
    "        \n",
    "        # restart with certain prob\n",
    "        x = rdm.random()\n",
    "        if x < restart_prob:\n",
    "            currentnode = startnode\n",
    "        else:    \n",
    "            # move a step forward\n",
    "            nextnode = rdm.sample(list(und_graph[currentnode]),1)[0]\n",
    "            currentnode = nextnode\n",
    "        \n",
    "        # find a new startnode if number of nodes in sample does not grow\n",
    "        if restart_iteration < jump_iteration:\n",
    "            restart_iteration += 1\n",
    "        else:\n",
    "            if last_number_of_nodes == len(nodelist):\n",
    "                startnode = rdm.sample(und_graph.nodes(),1)[0]\n",
    "                currentnode = startnode\n",
    "            restart_iteration = 0\n",
    "            last_number_of_nodes = len(nodelist)\n",
    "    return und_graph.subgraph(nodelist) ,total_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest Fire Sampling\n",
    "def forest_fire_sampling(und_graph, sample_percentage, burn_prob, seed = None):\n",
    "    # set random seed\n",
    "    rdm.seed(seed)\n",
    "    \n",
    "    # sample size round down to interger\n",
    "    sample_size = int(nx.number_of_nodes(und_graph) * sample_percentage)\n",
    "    \n",
    "    # set starting node\n",
    "    startnode = rdm.sample(und_graph.nodes(),1)[0]\n",
    "    currentnode = startnode\n",
    "    explore_list = queue.Queue(maxsize = nx.number_of_nodes(und_graph))\n",
    "    explore_list.put(currentnode)\n",
    "    \n",
    "    # result node set and total iteration\n",
    "    nodelist = set()\n",
    "    visitedlist = set()\n",
    "    total_iteration = 0\n",
    "    \n",
    "    while len(nodelist) < sample_size:\n",
    "        visitedlist.add(currentnode)\n",
    "        total_iteration += 1\n",
    "        \n",
    "        # set new nodes in queue if empty\n",
    "        if explore_list.empty():\n",
    "            currentnode = rdm.sample(und_graph.nodes(),1)[0]\n",
    "            if currentnode not in nodelist:\n",
    "                explore_list.put(currentnode)\n",
    "        else:\n",
    "            # dequeue node and explore its neighbor\n",
    "            currentnode = explore_list.get()\n",
    "            nodelist.add(currentnode)\n",
    "            for burn_node in list(und_graph[currentnode]):\n",
    "                if burn_node not in visitedlist:\n",
    "                    x = rdm.random()\n",
    "                    if x < burn_prob:\n",
    "                        explore_list.put(burn_node)\n",
    "                        visitedlist.add(burn_node)\n",
    "        \n",
    "    return und_graph.subgraph(nodelist), total_iteration\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest Fire Sampling(Directed)\n",
    "# backward_burn_prob = forward_burn_prob/backward_burn_prob_ratio (which is an interger in most cases)\n",
    "def directed_forest_fire_sampling(di_graph, sample_percentage, forward_burn_prob, backward_burn_prob_ratio, seed = None):\n",
    "    # set random seed\n",
    "    rdm.seed(seed)\n",
    "    \n",
    "    # sample size round down to interger\n",
    "    sample_size = int(nx.number_of_nodes(di_graph) * sample_percentage)\n",
    "    backward_burn_prob = forward_burn_prob/backward_burn_prob_ratio\n",
    "    \n",
    "    # set starting node\n",
    "    startnode = rdm.sample(di_graph.nodes(),1)[0]\n",
    "    currentnode = startnode\n",
    "    explore_list = queue.Queue(maxsize = nx.number_of_nodes(di_graph))\n",
    "    explore_list.put(currentnode)\n",
    "    \n",
    "    # result node set and total iteration\n",
    "    nodelist = set()\n",
    "    visitedlist = set()\n",
    "    total_iteration = 0\n",
    "    \n",
    "    while len(nodelist) < sample_size:\n",
    "        visitedlist.add(currentnode)\n",
    "        total_iteration += 1\n",
    "        \n",
    "        # set new nodes in queue if empty\n",
    "        if explore_list.empty():\n",
    "            currentnode = rdm.sample(di_graph.nodes(),1)[0]\n",
    "            if currentnode not in nodelist:\n",
    "                explore_list.put(currentnode)\n",
    "        else:\n",
    "            # dequeue node and explore its neighbor\n",
    "            currentnode = explore_list.get()\n",
    "            nodelist.add(currentnode)\n",
    "            # forward burn\n",
    "            for forward_burn_node in list(di_graph[currentnode]):\n",
    "                if forward_burn_node not in visitedlist:\n",
    "                    x = rdm.random()\n",
    "                    if x < forward_burn_prob:\n",
    "                        explore_list.put(forward_burn_node)\n",
    "                        visitedlist.add(forward_burn_node)\n",
    "            # backward burn            \n",
    "            for backward_burn_node in list(di_graph.predecessors(currentnode)):\n",
    "                if backward_burn_node not in visitedlist:\n",
    "                    y = rdm.random()\n",
    "                    if y < backward_burn_prob:\n",
    "                        explore_list.put(backward_burn_node)\n",
    "                        visitedlist.add(backward_burn_node)\n",
    "        \n",
    "    return di_graph.subgraph(nodelist) ,total_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# powerlaw.pdf(x, k) = c * x**(-k) default k = 2\n",
    "# 0<a<1 in degree distribution\n",
    "# low_degree and high_degree are intergers\n",
    "# return largest connected component\n",
    "def powerlaw_degree_graph_generator(n, a, seed = None):\n",
    "    rdm.seed(seed)\n",
    "    r = nx.utils.powerlaw_sequence(n, exponent = a)\n",
    "    int_r = list(map(int, r))\n",
    "    if sum(int_r)%2 == 1:\n",
    "        int_r[0] = int_r[0] + 1\n",
    "    G = nx.configuration_model(int_r,create_using = nx.Graph(), seed = seed)\n",
    "    cc_list = sorted(list(nx.connected_components(G)), key = len, reverse = True)\n",
    "    Gc = G.subgraph(cc_list[0])\n",
    "    return Gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_g, iteration = Frontier_sampling(g, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33486 77342\n"
     ]
    }
   ],
   "source": [
    "print(nx.number_of_nodes(sub_g), nx.number_of_edges(sub_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frontier Sampling\n",
    "def Frontier_sampling(und_graph, sample_percentage, random_walker = 5, seed = None):\n",
    "    # set random seed\n",
    "    rdm.seed(seed)\n",
    "    \n",
    "    # sample size round down to interger\n",
    "    sample_size = int(nx.number_of_nodes(und_graph) * sample_percentage)\n",
    "    \n",
    "    # set starting node\n",
    "    current_walker = rdm.sample(und_graph.nodes(), random_walker)\n",
    "    # initialize degrees\n",
    "    degree_list = [und_graph.degree[n] for n in current_walker]\n",
    "    \n",
    "    # result node set and total iteration\n",
    "    nodelist = set()\n",
    "    total_iteration = 0\n",
    "    for cn in current_walker:\n",
    "        nodelist.add(cn)\n",
    "    \n",
    "    while len(nodelist) < sample_size:\n",
    "        # add current node\n",
    "        total_iteration += 1\n",
    "        currentnode, index = Frontier_sampling_select_node(current_walker, degree_list) # select walker dependently\n",
    "        \n",
    "        # move a step forward\n",
    "        nextnode = rdm.sample(list(und_graph[currentnode]),1)[0]\n",
    "        nodelist.add(nextnode)\n",
    "        current_walker[index] = nextnode\n",
    "        degree_list[index] = und_graph.degree[nextnode]\n",
    "    \n",
    "    return und_graph.subgraph(nodelist), total_iteration #, visited_time \n",
    "\n",
    "def Frontier_sampling_select_node(current_walker, degree_list):\n",
    "    probability_distribution = [d/sum(degree_list) for d in degree_list]\n",
    "    select_node = choice(current_walker, 1, p = probability_distribution)[0]\n",
    "    return select_node, current_walker.index(select_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis–Hastings random walk sampling\n",
    "def MH_random_walk_sampling(und_graph, sample_percentage, jump_iteration = 100, seed = None):\n",
    "    # set random seed\n",
    "    rdm.seed(seed)\n",
    "    \n",
    "    # sample size round down to interger\n",
    "    sample_size = int(nx.number_of_nodes(und_graph) * sample_percentage)\n",
    "    \n",
    "    # set starting node\n",
    "    startnode = rdm.sample(und_graph.nodes(),1)[0]\n",
    "    currentnode = startnode\n",
    "    \n",
    "    # used for jump when no new node visited in certain iteration\n",
    "    restart_iteration = 0 \n",
    "    last_number_of_nodes = 0\n",
    "    \n",
    "    # result node set and total iteration\n",
    "    nodelist = set()\n",
    "    total_iteration = 0\n",
    "    \n",
    "    while len(nodelist) < sample_size:\n",
    "        # add current node\n",
    "        total_iteration += 1\n",
    "        nodelist.add(currentnode)        \n",
    "        \n",
    "        # MHRW process\n",
    "        random_number = rdm.random()\n",
    "        neighbors = list(und_graph[currentnode])\n",
    "        list_of_candidates = neighbors[:]\n",
    "        list_of_candidates.append(currentnode)\n",
    "        probability_distribution = []        \n",
    "        for n in neighbors:\n",
    "            p_n = 1/und_graph.degree[currentnode] * min(1, und_graph.degree[currentnode]/und_graph.degree[n])\n",
    "            probability_distribution.append(p_n)\n",
    "        total = sum(probability_distribution)\n",
    "        if total > 1:\n",
    "            total = 1\n",
    "        probability_distribution.append(1 - total)\n",
    "        currentnode = choice(list_of_candidates, 1, p = probability_distribution)[0]\n",
    "        \n",
    "        # find a new startnode if number of nodes in sample does not grow\n",
    "        if restart_iteration < jump_iteration:\n",
    "            restart_iteration += 1\n",
    "        else:\n",
    "            if last_number_of_nodes == len(nodelist):\n",
    "                startnode = rdm.sample(und_graph.nodes(),1)[0]\n",
    "                currentnode = startnode\n",
    "            restart_iteration = 0\n",
    "            last_number_of_nodes = len(nodelist)\n",
    "    return und_graph.subgraph(nodelist) ,total_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis–Hastings random walk sampling\n",
    "def MH_random_walk_sampling_di(di_graph, sample_percentage, jump_iteration = 100, seed = None):\n",
    "    # transfer graph to undirected\n",
    "    und_graph = di_graph.to_undirected()\n",
    "    \n",
    "    # set random seed\n",
    "    rdm.seed(seed)\n",
    "    \n",
    "    # sample size round down to interger\n",
    "    sample_size = int(nx.number_of_nodes(und_graph) * sample_percentage)\n",
    "    \n",
    "    # set starting node\n",
    "    startnode = rdm.sample(und_graph.nodes(),1)[0]\n",
    "    currentnode = startnode\n",
    "    \n",
    "    # used for jump when no new node visited in certain iteration\n",
    "    restart_iteration = 0 \n",
    "    last_number_of_nodes = 0\n",
    "    \n",
    "    # result node set and total iteration\n",
    "    nodelist = set()\n",
    "    total_iteration = 0\n",
    "    \n",
    "    while len(nodelist) < sample_size:\n",
    "        # add current node\n",
    "        total_iteration += 1\n",
    "        nodelist.add(currentnode)        \n",
    "        \n",
    "        # MHRW process\n",
    "        random_number = rdm.random()\n",
    "        neighbors = list(und_graph[currentnode])\n",
    "        list_of_candidates = neighbors[:]\n",
    "        list_of_candidates.append(currentnode)\n",
    "        probability_distribution = []        \n",
    "        for n in neighbors:\n",
    "            p_n = 1/und_graph.degree[currentnode] * min(1, und_graph.degree[currentnode]/und_graph.degree[n])\n",
    "            probability_distribution.append(p_n)\n",
    "        total = sum(probability_distribution)\n",
    "        if total > 1:\n",
    "            total = 1\n",
    "        probability_distribution.append(1 - total)\n",
    "        currentnode = choice(list_of_candidates, 1, p = probability_distribution)[0]\n",
    "        \n",
    "        # find a new startnode if number of nodes in sample does not grow\n",
    "        if restart_iteration < jump_iteration:\n",
    "            restart_iteration += 1\n",
    "        else:\n",
    "            if last_number_of_nodes == len(nodelist):\n",
    "                startnode = rdm.sample(und_graph.nodes(),1)[0]\n",
    "                currentnode = startnode\n",
    "            restart_iteration = 0\n",
    "            last_number_of_nodes = len(nodelist)\n",
    "    return di_graph.subgraph(nodelist) ,total_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random walk + attributes sampling\n",
    "# alpha from (0,1) uniform probability that node being selected from attribute bucket\n",
    "def random_walk_attribute_sampling(graph, percentage, attribute_list, attribute_bucket, alpha, jump_iteration = 100, seed = None):\n",
    "    # set random seed\n",
    "    rdm.seed(seed)\n",
    "    \n",
    "    sample_size = round(nx.number_of_nodes (graph)*percentage)\n",
    "    visitednodes = set()\n",
    "    samplednodes = set()\n",
    "    Gc = set()\n",
    "    \n",
    "    startnode = rdm.sample(graph.nodes(),1)[0]\n",
    "    currentnode = startnode\n",
    "    iteration = 0 \n",
    "    total_iteration = 0\n",
    "    sample_full = False\n",
    "    last_number_of_nodes = 0\n",
    "    \n",
    "    while not sample_full:\n",
    "        total_iteration += 1\n",
    "        if currentnode not in visitednodes:\n",
    "            # add currentnode\n",
    "            visitednodes.add(currentnode)\n",
    "            samplednodes.add(currentnode)\n",
    "\n",
    "            # count 1st components\n",
    "            if len(samplednodes) > sample_size:\n",
    "                G = graph.subgraph(samplednodes)\n",
    "                #Gc = max(nx.weakly_connected_components(G), key=len)\n",
    "                Gc = max(nx.connected_components(G), key=len)\n",
    "                # decide whether terminate\n",
    "                if len(Gc) >= sample_size:\n",
    "                    sample_full = True\n",
    "                else:\n",
    "                    samplednodes = Gc\n",
    "\n",
    "            # add same attribute nodes from bucket\n",
    "            if (not sample_full) and (currentnode in attribute_list.keys()):\n",
    "                bucketnodes = bucketsampling(currentnode, attribute_list, attribute_bucket, alpha)\n",
    "                for bn in bucketnodes:                \n",
    "                    samplednodes.add(bn)        \n",
    "                    \n",
    "                    \n",
    "        # add edge when currentnode is not a sink (not needed for undirected graph)\n",
    "        if len(list(graph[currentnode])) > 0:\n",
    "            nextnode = rdm.sample(list(graph[currentnode]),1)[0]\n",
    "            currentnode = nextnode\n",
    "        # jump back to the start node when reach a sink    \n",
    "        else:\n",
    "            currentnode = startnode\n",
    "\n",
    "        # find a new startnode if number of nodes in sample does not grow\n",
    "        if iteration < jump_iteration:\n",
    "            iteration += 1\n",
    "        else:\n",
    "            if last_number_of_nodes == len(visitednodes):\n",
    "                startnode = rdm.sample(graph.nodes(),1)[0]\n",
    "                currentnode = startnode\n",
    "            iteration = 0\n",
    "            last_number_of_nodes = len(visitednodes)\n",
    "                        \n",
    "    G = graph.subgraph(samplednodes)\n",
    "    #Gc = max(nx.weakly_connected_components(G), key=len) \n",
    "    Gc = max(nx.connected_components(G), key=len)\n",
    "    return G.subgraph(Gc), total_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random attributes sampling\n",
    "# Random select an attribute based on attribute distribution, sample from this bukcet with prob alpha\n",
    "def random_attribute_sampling(graph, percentage, attribute_list, attribute_bucket, alpha):\n",
    "    sample_size = round(nx.number_of_nodes (graph)*percentage)\n",
    "    samplednodes = set()\n",
    "    sample_full = False\n",
    "    samplednodes_candidate = rdm.sample(graph.nodes(), k = round(nx.number_of_nodes(graph)*percentage))\n",
    "    rdm.shuffle(samplednodes_candidate)\n",
    "    i = 0\n",
    "    \n",
    "    while not sample_full:\n",
    "        currentnode = samplednodes_candidate[i]\n",
    "        i+=1\n",
    "        if currentnode in attribute_list.keys():\n",
    "            bucketnodes = bucketsampling(currentnode, attribute_list, attribute_bucket, alpha)\n",
    "            for bn in bucketnodes:\n",
    "                if len(samplednodes) < sample_size:\n",
    "                    samplednodes.add(bn)\n",
    "                else:\n",
    "                    sample_full = True\n",
    "        else:\n",
    "            if len(samplednodes) < sample_size:\n",
    "                samplednodes.add(currentnode)\n",
    "            else:\n",
    "                sample_full = True\n",
    "                \n",
    "    return graph.subgraph(samplednodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucketsampling(node, attribute_list, attribute_bucket, alpha):\n",
    "    samplednode = []\n",
    "    for i in attribute_list[node]:\n",
    "        for n in attribute_bucket[i]:\n",
    "            random_number = rdm.random()\n",
    "            if random_number <= alpha:\n",
    "                samplednode.append(n)\n",
    "    rdm.shuffle(samplednode)\n",
    "    return samplednode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function used by random_walk_attribute_sampling\n",
    "def bucketsampling_facebook(node, attribute_list, attribute_bucket, alpha):\n",
    "    samplednode = []\n",
    "    first_pair = (attribute_list[node][0], attribute_list[node][2])\n",
    "    for node in attribute_bucket[first_pair]:\n",
    "        random_number = rdm.random()\n",
    "        if random_number <= alpha:\n",
    "            samplednode.append(node)\n",
    "    if attribute_list[node][1] != 0:\n",
    "        second_pair = (attribute_list[node][1], attribute_list[node][2])\n",
    "        for node in attribute_bucket[second_pair]:\n",
    "            random_number = rdm.random()\n",
    "            if random_number <= alpha:\n",
    "                samplednode.append(node)\n",
    "    rdm.shuffle(samplednode)\n",
    "    return samplednode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measuring assortativity\n",
    "def assortativity(graph, attibute_list):\n",
    "    nx.set_node_attributes(graph, attibute_list, 'attribute')\n",
    "    ast = nx.attribute_assortativity_coefficient(graph, 'attribute')\n",
    "    return ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulate al for facebook-100\n",
    "def fitst_attribute(attribute_list):\n",
    "    new_al = {}\n",
    "    for i in attribute_list.keys():\n",
    "        new_al[i] = (attribute_list[i][0], attribute_list[i][2])\n",
    "    return new_al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree distribution generalized by number of nodes\n",
    "def degree_distribution(graph):\n",
    "    degrees = {}\n",
    "    generalizer = 1/nx.number_of_nodes (graph)\n",
    "    for n in graph.nodes():\n",
    "        degree = graph.degree(n)\n",
    "        if degree not in degrees:\n",
    "            degrees[degree] = 0\n",
    "        degrees[degree] += generalizer\n",
    "    return dict(sorted(degrees.items()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdegree distribution \n",
    "def outdegree_distribution(digraph):\n",
    "    degrees = {}\n",
    "    generalizer = 1/nx.number_of_nodes (digraph)\n",
    "    for n in digraph.nodes():\n",
    "        degree = digraph.out_degree(n)\n",
    "        if degree not in degrees:\n",
    "            degrees[degree] = 0\n",
    "        degrees[degree] += generalizer\n",
    "    return dict(sorted(degrees.items()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdegree distribution \n",
    "def indegree_distribution(digraph):\n",
    "    degrees = {}\n",
    "    generalizer = 1/nx.number_of_nodes (digraph)\n",
    "    for n in digraph.nodes():\n",
    "        degree = digraph.in_degree(n)\n",
    "        if degree not in degrees:\n",
    "            degrees[degree] = 0\n",
    "        degrees[degree] += generalizer\n",
    "    return dict(sorted(degrees.items()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CC distribution \n",
    "def cc_distribution(graph):\n",
    "    cc_list = sorted(list(nx.connected_components(graph)), key = len, reverse=True)\n",
    "    sizes = {}\n",
    "    total = nx.number_of_nodes(graph)\n",
    "    rank = 1\n",
    "    for n in cc_list:\n",
    "        sizes[rank] = len(n)/total\n",
    "        rank += 1\n",
    "    return dict(list(sizes.items()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCC distribution \n",
    "def scc_distribution(digraph):\n",
    "    scc_list = sorted(list(nx.strongly_connected_components(digraph)), key = len, reverse=True)\n",
    "    sizes = {}\n",
    "    total = nx.number_of_nodes(digraph)\n",
    "    rank = 1\n",
    "    for n in scc_list:\n",
    "        sizes[rank] = len(n)/total\n",
    "        rank += 1\n",
    "    return dict(list(sizes.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WCC distribution \n",
    "def wcc_distribution(digraph):\n",
    "    wcc_list = sorted(list(nx.weakly_connected_components(digraph)), key = len, reverse=True)\n",
    "    sizes = {}\n",
    "    total = nx.number_of_nodes(digraph)\n",
    "    rank = 1\n",
    "    for n in wcc_list:\n",
    "        sizes[rank] = len(n)/total\n",
    "        rank += 1\n",
    "    return dict(list(sizes.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering coefficient distribution\n",
    "def clustering_coefficient_distribution(graph):\n",
    "    clust_coefficients = nx.clustering(graph)\n",
    "    coeff = {}\n",
    "    count = {}\n",
    "    for n in graph:\n",
    "        degree = graph.degree(n)\n",
    "        coefficient = clust_coefficients[n]\n",
    "        if degree not in coeff:\n",
    "            coeff[degree] = 0\n",
    "            count[degree] = 0\n",
    "        coeff[degree] += coefficient\n",
    "        count[degree] += 1\n",
    "    for key in coeff:\n",
    "        coeff[key] = coeff[key]/count[key]\n",
    "    return dict(sorted(coeff.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average absolute distance\n",
    "def average_absolute_distance(cc1, cc2):\n",
    "    result = []\n",
    "    for i in cc1.keys():\n",
    "        if i in cc2.keys():\n",
    "            result.append(abs(cc1[i]-cc2[i]))\n",
    "    return sum(result)/len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer directed graph to undirected\n",
    "def to_undirected(digraph):\n",
    "    return digraph.to_undirected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Distance distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Min-Distance distribution\\n# This part was disabled, we use ANF to approximate due to size of the graphs\\n# We directly read from files, since snap.py package only supports python 2.7\\n\\ndef min_distance_distribution(graph):\\n    min_distance = {}\\n    for n in graph.nodes():\\n        start_time = time.time()\\n        length = nx.single_source_dijkstra_path_length(graph, n)\\n        for p in length:\\n            if p not in min_distance:\\n                min_distance[p] = 0\\n            min_distance[p] += 1\\n        #print(\"--- %s seconds ---\" % (time.time() - start_time))\\n    total = sum(min_distance.values())\\n\\n    for k in min_distance:\\n        min_distance[k] = min_distance[k]/total\\n    return sorted(min_distance.items())\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Min-Distance distribution\n",
    "# This part was disabled, we use ANF to approximate due to size of the graphs\n",
    "# We directly read from files, since snap.py package only supports python 2.7\n",
    "\n",
    "def min_distance_distribution(graph):\n",
    "    min_distance = {}\n",
    "    for n in graph.nodes():\n",
    "        start_time = time.time()\n",
    "        length = nx.single_source_dijkstra_path_length(graph, n)\n",
    "        for p in length:\n",
    "            if p not in min_distance:\n",
    "                min_distance[p] = 0\n",
    "            min_distance[p] += 1\n",
    "        #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    total = sum(min_distance.values())\n",
    "\n",
    "    for k in min_distance:\n",
    "        min_distance[k] = min_distance[k]/total\n",
    "    return sorted(min_distance.items())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_plothop_distribution(filename):    \n",
    "    plothop = load_distribution(filename)\n",
    "    min_distance = {}\n",
    "    min_distance[0] = plothop[0]\n",
    "    for i in plothop.keys():\n",
    "        if i > 0:\n",
    "            min_distance[i] = plothop[i] - plothop[i-1]\n",
    "    for i in min_distance.keys():\n",
    "        min_distance[i] /= plothop[len(plothop) - 1]\n",
    "    return min_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First left singular vector and singular values distribution\n",
    "def get_SVD_distribution(graph, rank):\n",
    "    start_time = time.time()\n",
    "\n",
    "    A = nx.adj_matrix(graph).asfptype()\n",
    "    U, s, Vh = linalg.svds(A, k = rank)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    s_sorted = np.flipud(s)\n",
    "    firstU_sorted = np.flipud(np.sort(np.absolute(U[:,0])))\n",
    "\n",
    "    FLSV = {}\n",
    "    SV = {}\n",
    "    \n",
    "    for i in range(len(s_sorted)):\n",
    "        SV[i] = s_sorted[i]\n",
    "    for i in range(len(firstU_sorted)):\n",
    "        FLSV[i] = firstU_sorted[i]\n",
    "        \n",
    "    total = sum(SV.values())\n",
    "    for i in SV.keys():\n",
    "        SV[i] /= total\n",
    "\n",
    "    return dict(list(FLSV.items())), dict(list(SV.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale make it compareable\n",
    "def sample_FLSV_distribution(flsv, percentage):\n",
    "    new_flsv = {}\n",
    "    skip = int(1/percentage)\n",
    "    for i in range(round(len(flsv)*percentage)):\n",
    "        new_flsv[i] = flsv[i*skip]\n",
    "    total = sum(new_flsv.values())\n",
    "    for i in new_flsv.keys():\n",
    "        new_flsv[i] /= total\n",
    "    return new_flsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring Attributes Cover Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes form selected attribute/ all nodes\n",
    "def attribute_cover_rate(subgraph, attribute_list, attribute_bucket):\n",
    "    sampled_attributes = set()\n",
    "    for i in subgraph.nodes():\n",
    "        if i in attribute_list:\n",
    "            for j in attribute_list[i]:\n",
    "                sampled_attributes.add(j)\n",
    "    sampled_attributed_nodes = set()\n",
    "    for i in sampled_attributes:\n",
    "        for j in attribute_bucket[i]:\n",
    "            sampled_attributed_nodes.add(j)\n",
    "    rate = len(sampled_attributed_nodes)/len(attribute_list)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random walk sampling\n",
    "def random_walk_attribute_cover_rate(graph, ACR, attribute_list, attribute_bucket):\n",
    "    startnode = rdm.sample(graph.nodes(),1)[0]\n",
    "    currentnode = startnode\n",
    "    iteration = 0 \n",
    "    last_number_of_nodes = 0\n",
    "    nodelist = set()\n",
    "    total_iteration = 0\n",
    "    acr = 0\n",
    "    sampled_attributes = set()\n",
    "    while acr < ACR:\n",
    "        total_iteration += 1\n",
    "        if currentnode not in nodelist:\n",
    "            nodelist.add(currentnode)\n",
    "            if currentnode in attribute_list.keys():\n",
    "                for i in attribute_list[currentnode]:\n",
    "                    if i not in sampled_attributes:\n",
    "                        sampled_attributes.add(i)\n",
    "                        acr += len(attribute_bucket[i])/nx.number_of_nodes(graph)\n",
    "            \n",
    "            \n",
    "        # 15% probability jump back to startnode\n",
    "        x = rdm.randint(1,100)\n",
    "        if x < 16:\n",
    "            currentnode = startnode\n",
    "            \n",
    "        # add edge when currentnode is not a sink\n",
    "        if len(list(graph[currentnode])) > 0:\n",
    "            nextnode = rdm.sample(list(graph[currentnode]),1)[0]\n",
    "            currentnode = nextnode\n",
    "        # jump back to the start node when reach a sink    \n",
    "        else:\n",
    "            currentnode = startnode\n",
    "            \n",
    "        # find a new startnode if number of nodes in sample does not grow\n",
    "        if iteration < 10:\n",
    "            iteration += 1\n",
    "        else:\n",
    "            if last_number_of_nodes == len(nodelist):\n",
    "                startnode = rdm.sample(graph.nodes(),1)[0]\n",
    "                currentnode = startnode\n",
    "            iteration = 0\n",
    "            last_number_of_nodes = len(nodelist)\n",
    "    return total_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampled chance via Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampled_chance_via_degree(graph, samples_path, numbers):\n",
    "    sample = []\n",
    "    result = {}\n",
    "    for i in range(1,numbers+1):\n",
    "        sample.append(set(nx.read_adjlist(samples_path + str(i) +\".adjlist.gz\", create_using=nx.DiGraph()).nodes))\n",
    "    for n in graph.nodes:\n",
    "        count = 0\n",
    "        for i in range(0,numbers):\n",
    "            if n in sample[i]:\n",
    "                count+=1\n",
    "        if graph.in_degree[n] not in result.keys():\n",
    "            result[graph.in_degree[n]] = [count/numbers]\n",
    "        else:\n",
    "            result[graph.in_degree[n]].append(count/numbers)\n",
    "        \n",
    "    result2 = {}\n",
    "    for i in result.keys():\n",
    "        result2[i] = sum(result[i])/len(result[i])\n",
    "    return dict(sorted(result2.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measuring covarance between any pair of samples\n",
    "def duplicates_among_samples(path, numbers):\n",
    "    sample = []\n",
    "    for i in range(1,numbers+1):\n",
    "        sample.append(nx.read_adjlist(path + str(i) +\".adjlist.gz\"))\n",
    "    result = []\n",
    "    n_nodes = len(sample[0].nodes)\n",
    "    for i in range(0,numbers):\n",
    "        for j in range(i+1, numbers):\n",
    "            a = set(sample[i].nodes)\n",
    "            b = set(sample[j].nodes)\n",
    "            result.append(2-len(a|b)/n_nodes)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save distribtuions into file\n",
    "def save_distribution(distribution, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for k in distribution.keys():\n",
    "            f.write(\"%s\\t%s\\n\" % (k, distribution[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load distribtuions from file\n",
    "def load_distribution(path):\n",
    "    distribution = {}\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line[0] == '#':\n",
    "                continue\n",
    "            item = line.strip().split('\\t')\n",
    "            distribution[int(item[0])] = float(item[1])\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JS-divergence of two distribtuions\n",
    "def JS_divergence(d1,d2):\n",
    "    dit_d1 = dit.ScalarDistribution(list(d1.keys()), list(d1.values()))\n",
    "    dit_d2 = dit.ScalarDistribution(list(d2.keys()), list(d2.values()))\n",
    "    return jensen_shannon_divergence([dit_d1, dit_d2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another implementaion of js-divergence\n",
    "def JSD(P, Q):\n",
    "    Pv = list(P.values())\n",
    "    Qv = list(Q.values())\n",
    "    _P = Pv / norm(Pv, ord=1)\n",
    "    _Q = Qv / norm(Qv, ord=1)\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer adjlist to edgelist for plothop in snap.py\n",
    "def adjlist_to_edgelist(path_subgraphs):\n",
    "    # get all files from path_subgraphs\n",
    "    list_subgraphs = listdir(path_subgraphs)\n",
    "    for sg in list_subgraphs:\n",
    "        #print(sg)\n",
    "        #print(path_subgraphs + sg)\n",
    "        #subgraph = nx.read_adjlist(path_subgraphs + sg, create_using=nx.DiGraph())\n",
    "        subgraph = nx.read_adjlist(path_subgraphs + sg)\n",
    "        subgraph_name = re.search('(.+?)\\.adjlist\\.gz', sg).group(1)        \n",
    "        nx.write_edgelist(subgraph, path_subgraphs + subgraph_name + \".edgelist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undirected graph\n",
    "# do all evaluations at one time, and save distribtuions\n",
    "def do_all_evaluations_udg(graph, path_subgraphs):\n",
    "    # evaluations for graph\n",
    "    degree_distribution_g = degree_distribution(graph)\n",
    "    save_distribution(degree_distribution_g, \"degree_distribution_full\")\n",
    "    \n",
    "    clustering_coefficient_distribution_g = clustering_coefficient_distribution(graph)\n",
    "    save_distribution(clustering_coefficient_distribution_g, \"clustering_coefficient_distribution_full\")\n",
    "    \n",
    "    min_distance_distribution_g = read_plothop_distribution(\"hop.tab\")\n",
    "    \n",
    "    cc_distribution_g = cc_distribution(graph)\n",
    "    save_distribution(cc_distribution_g, \"connected_components_distribution_full\")\n",
    "    \n",
    "    FLSV_g, SV_g = get_SVD_distribution(graph, 100)\n",
    "    save_distribution(FLSV_g, \"FLSV_full\")\n",
    "    save_distribution(SV_g, \"SV_full\")\n",
    "    \n",
    "    # get all files from path_subgraphs\n",
    "    list_subgraphs = listdir(path_subgraphs)\n",
    "    \n",
    "    # JS divergence for different methods\n",
    "    JS_degree = []\n",
    "    MAE_clustering = []\n",
    "    JS_mindistance = []\n",
    "    JS_cc = []\n",
    "    #JS_FLSV = []\n",
    "    JS_SV = []\n",
    "    \n",
    "    for sg in list_subgraphs:\n",
    "        print(sg)\n",
    "        subgraph = nx.read_adjlist(path_subgraphs + sg)\n",
    "        subgraph_name = re.search('(.+?)\\.adjlist\\.gz', sg).group(1)\n",
    "        \n",
    "        degree_distribution_subg = degree_distribution(subgraph)\n",
    "        save_distribution(degree_distribution_subg, \"degree_distribution_\" + subgraph_name)\n",
    "        js_degree_distribution = JS_divergence(degree_distribution_g, degree_distribution_subg)\n",
    "        JS_degree.append(js_degree_distribution)\n",
    "        \n",
    "        clustering_coefficient_distribution_subg = clustering_coefficient_distribution(subgraph)\n",
    "        save_distribution(clustering_coefficient_distribution_g, \"clustering_coefficient_distribution_\" + subgraph_name)\n",
    "        absolute_error_clustering_coefficient = average_absolute_distance(clustering_coefficient_distribution_g, clustering_coefficient_distribution_subg)\n",
    "        MAE_clustering.append(absolute_error_clustering_coefficient)\n",
    "        \n",
    "        min_distance_distribution_subg = read_plothop_distribution(\"hop.\" + subgraph_name + \".tab\")\n",
    "        js_min_distance_distribution = JS_divergence(min_distance_distribution_g, min_distance_distribution_subg)\n",
    "        JS_mindistance.append(js_min_distance_distribution)\n",
    "        \n",
    "        cc_distribution_subg = cc_distribution(subgraph)\n",
    "        save_distribution(cc_distribution_subg, \"connected_components_distribution_\" + subgraph_name)\n",
    "        js_cc_distribution = JS_divergence(cc_distribution_g, cc_distribution_subg)\n",
    "        JS_cc.append(js_cc_distribution)\n",
    "        \n",
    "        FLSV_subg, SV_subg = get_SVD_distribution(subgraph, 100)\n",
    "        save_distribution(FLSV_subg, \"FLSV_\" + subgraph_name)\n",
    "        save_distribution(SV_subg, \"SV_\" + subgraph_name)\n",
    "        #js_FLSV = JSD(sample_FLSV_distribution(FLSV_g, 0.1), FLSV_subg) # here JS_divergence gives bad normalization\n",
    "        js_SV = JS_divergence(SV_g, SV_subg)\n",
    "        #JS_FLSV.append(js_FLSV)\n",
    "        JS_SV.append(js_SV)\n",
    "    \n",
    "    print(\"--------------------degree--------------------\")\n",
    "    print(JS_degree)\n",
    "    print(sum(JS_degree[0:10])/10)\n",
    "    print(sum(JS_degree[10:20])/10)\n",
    "    print(sum(JS_degree[20:30])/10)\n",
    "    print(sum(JS_degree[30:40])/10)\n",
    "    print(\"--------------------clustering coefficient--------------------\")\n",
    "    print(MAE_clustering)\n",
    "    print(sum(MAE_clustering[0:10])/10)\n",
    "    print(sum(MAE_clustering[10:20])/10)\n",
    "    print(sum(MAE_clustering[20:30])/10)\n",
    "    print(sum(MAE_clustering[30:40])/10)\n",
    "    print(\"--------------------min distance--------------------\")\n",
    "    print(JS_mindistance)\n",
    "    print(sum(JS_mindistance[0:10])/10)\n",
    "    print(sum(JS_mindistance[10:20])/10)\n",
    "    print(sum(JS_mindistance[20:30])/10)\n",
    "    print(sum(JS_mindistance[30:40])/10)\n",
    "    print(\"--------------------connected components--------------------\")\n",
    "    print(JS_cc)\n",
    "    print(sum(JS_cc[0:10])/10)\n",
    "    print(sum(JS_cc[10:20])/10)\n",
    "    print(sum(JS_cc[20:30])/10)\n",
    "    print(sum(JS_cc[30:40])/10)\n",
    "    '''\n",
    "    print(\"--------------------FLSV--------------------\")\n",
    "    print(JS_FLSV)\n",
    "    print(sum(JS_FLSV[0:10])/10)\n",
    "    print(sum(JS_FLSV[10:20])/10)\n",
    "    print(sum(JS_FLSV[20:30])/10)\n",
    "    print(sum(JS_FLSV[30:40])/10)\n",
    "    '''\n",
    "    print(\"--------------------SV--------------------\")\n",
    "    print(JS_SV)\n",
    "    print(sum(JS_SV[0:10])/10)\n",
    "    print(sum(JS_SV[10:20])/10)\n",
    "    print(sum(JS_SV[20:30])/10)\n",
    "    print(sum(JS_SV[30:40])/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 66.11482191085815 seconds ---\n",
      "rwa_1.adjlist.gz\n",
      "--- 5.094374418258667 seconds ---\n",
      "rwa_10.adjlist.gz\n",
      "--- 5.924156188964844 seconds ---\n",
      "rwa_2.adjlist.gz\n",
      "--- 4.67094349861145 seconds ---\n",
      "rwa_3.adjlist.gz\n",
      "--- 5.850487470626831 seconds ---\n",
      "rwa_4.adjlist.gz\n",
      "--- 5.848160982131958 seconds ---\n",
      "rwa_5.adjlist.gz\n",
      "--- 5.219184875488281 seconds ---\n",
      "rwa_6.adjlist.gz\n",
      "--- 4.691073656082153 seconds ---\n",
      "rwa_7.adjlist.gz\n",
      "--- 4.974052667617798 seconds ---\n",
      "rwa_8.adjlist.gz\n",
      "--- 5.4237542152404785 seconds ---\n",
      "rwa_9.adjlist.gz\n",
      "--- 5.0731360912323 seconds ---\n",
      "--------------------degree--------------------\n",
      "[0.04954893007415295, 0.03736560921341381, 0.05206248510942091, 0.036949909844233186, 0.05399349361814343, 0.035229917750017936, 0.04466005390411887, 0.03560760666211804, 0.02188586414756921, 0.017273992872789634]\n",
      "0.0384577863195978\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "--------------------clustering coefficient--------------------\n",
      "[0.030317978520353756, 0.028162268466548696, 0.02807355878902223, 0.024385187958539785, 0.025521961433500825, 0.020380831744180213, 0.026169111305670057, 0.024244128707332373, 0.014142994812897526, 0.015950267186346914]\n",
      "0.023734828892439237\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "--------------------min distance--------------------\n",
      "[0.00839136275622554, 0.04427305981024343, 0.04287860652570519, 0.04986173128348659, 0.019299541882308535, 0.018972623952538736, 0.04911573958822224, 0.02835222428474138, 0.04927063838783097, 0.08958691746747016]\n",
      "0.04000024459387728\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "--------------------connected components--------------------\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "--------------------SV--------------------\n",
      "[0.00023970347020529914, 9.990494833811425e-05, 0.00012908492645902214, 0.0001711485153617076, 0.00027020447420778027, 0.0003910998247809161, 0.0009249389983958523, 0.00014549688972831376, 0.0001727415864607451, 0.0007238297545963945]\n",
      "0.0003268153388534145\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "do_all_evaluations_udg(g, \"Dataset/Amazon/RATS/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directed graph\n",
    "# do all evaluations at one time, and save distribtuions\n",
    "def do_all_evaluations_dg(graph, path_subgraphs):\n",
    "    # evaluations for graph\n",
    "    \n",
    "    indegree_distribution_g = indegree_distribution(graph)\n",
    "    save_distribution(indegree_distribution_g, \"indegree_distribution_full\")\n",
    "    \n",
    "    outdegree_distribution_g = outdegree_distribution(graph)\n",
    "    save_distribution(outdegree_distribution_g, \"outdegree_distribution_full\")\n",
    "    \n",
    "    graph_ud = graph.to_undirected()\n",
    "    clustering_coefficient_distribution_g = clustering_coefficient_distribution(graph_ud)\n",
    "    save_distribution(clustering_coefficient_distribution_g, \"clustering_coefficient_distribution_full\")\n",
    "    \n",
    "    min_distance_distribution_g = read_plothop_distribution(\"hop.tab\")\n",
    "    \n",
    "    wcc_distribution_g = wcc_distribution(graph)\n",
    "    save_distribution(wcc_distribution_g, \"weakly_connected_components_distribution_full\")\n",
    "    \n",
    "    scc_distribution_g = scc_distribution(graph)\n",
    "    save_distribution(scc_distribution_g, \"strongly_connected_components_distribution_full\")\n",
    "    \n",
    "    FLSV_g, SV_g = get_SVD_distribution(graph, 100)\n",
    "    save_distribution(FLSV_g, \"FLSV_full\")\n",
    "    save_distribution(SV_g, \"SV_full\")\n",
    "    \n",
    "    # get all files from path_subgraphs\n",
    "    list_subgraphs = listdir(path_subgraphs)\n",
    "    \n",
    "\n",
    "    # JS divergence for different methods\n",
    "    JS_indegree = []\n",
    "    JS_outdegree = []\n",
    "    MAE_clustering = []\n",
    "    JS_mindistance = []\n",
    "    JS_wcc = []\n",
    "    JS_scc = []\n",
    "    JS_FLSV = []\n",
    "    JS_SV = []\n",
    "    \n",
    "    for sg in list_subgraphs:\n",
    "        print(sg)\n",
    "        subgraph = nx.read_adjlist(path_subgraphs + sg, create_using=nx.DiGraph())\n",
    "        subgraph_name = re.search('(.+?)\\.adjlist\\.gz', sg).group(1)\n",
    "        \n",
    "        # in case program crashes\n",
    "        '''\n",
    "        indegree_distribution_subg = load_distribution(\"indegree_distribution_\" + subgraph_name)\n",
    "        outdegree_distribution_subg = load_distribution(\"outdegree_distribution_\" + subgraph_name)\n",
    "        clustering_coefficient_distribution_subg = load_distribution(\"clustering_coefficient_distribution_\" + subgraph_name)\n",
    "        wcc_distribution_subg = load_distribution(\"weakly_connected_components_distribution_\" + subgraph_name)\n",
    "        scc_distribution_subg = load_distribution(\"strongly_connected_components_distribution_\" + subgraph_name)\n",
    "        FLSV_subg = load_distribution(\"FLSV_\" + subgraph_name)\n",
    "        SV_subg = load_distribution(\"SV_\" + subgraph_name)\n",
    "        '''        \n",
    "        \n",
    "        indegree_distribution_subg = indegree_distribution(subgraph)\n",
    "        save_distribution(indegree_distribution_subg, \"indegree_distribution_\" + subgraph_name)\n",
    "        js_indegree_distribution = JS_divergence(indegree_distribution_g, indegree_distribution_subg)\n",
    "        JS_indegree.append(js_indegree_distribution)\n",
    "        \n",
    "        outdegree_distribution_subg = outdegree_distribution(subgraph)\n",
    "        save_distribution(outdegree_distribution_subg, \"outdegree_distribution_\" + subgraph_name)\n",
    "        js_outdegree_distribution = JS_divergence(outdegree_distribution_g, outdegree_distribution_subg)\n",
    "        JS_outdegree.append(js_outdegree_distribution)\n",
    "        \n",
    "        clustering_coefficient_distribution_subg = clustering_coefficient_distribution(subgraph.to_undirected())\n",
    "        save_distribution(clustering_coefficient_distribution_subg, \"clustering_coefficient_distribution_\" + subgraph_name)\n",
    "        absolute_error_clustering_coefficient = average_absolute_distance(clustering_coefficient_distribution_g, clustering_coefficient_distribution_subg)\n",
    "        MAE_clustering.append(absolute_error_clustering_coefficient)\n",
    "        \n",
    "        min_distance_distribution_subg = read_plothop_distribution(\"hop.\" + subgraph_name + \".tab\")\n",
    "        js_min_distance_distribution = JS_divergence(min_distance_distribution_g, min_distance_distribution_subg)\n",
    "        JS_mindistance.append(js_min_distance_distribution)\n",
    "        \n",
    "        wcc_distribution_subg = wcc_distribution(subgraph)\n",
    "        save_distribution(wcc_distribution_subg, \"weakly_connected_components_distribution_\" + subgraph_name)\n",
    "        js_wcc_distribution = JS_divergence(wcc_distribution_g, wcc_distribution_subg)\n",
    "        JS_wcc.append(js_wcc_distribution)\n",
    "        \n",
    "        scc_distribution_subg = scc_distribution(subgraph)\n",
    "        save_distribution(scc_distribution_subg, \"strongly_connected_components_distribution_\" + subgraph_name)\n",
    "        js_scc_distribution = JS_divergence(scc_distribution_g, scc_distribution_subg)\n",
    "        JS_scc.append(js_scc_distribution)\n",
    "        \n",
    "        FLSV_subg, SV_subg = get_SVD_distribution(subgraph, 100)\n",
    "        save_distribution(FLSV_subg, \"FLSV_\" + subgraph_name)\n",
    "        save_distribution(SV_subg, \"SV_\" + subgraph_name)\n",
    "        js_FLSV = JSD(sample_FLSV_distribution(FLSV_g, 0.1), FLSV_subg) # here JS_divergence gives bad normalization\n",
    "        js_SV = JS_divergence(SV_g, SV_subg)\n",
    "        JS_FLSV.append(js_FLSV)\n",
    "        JS_SV.append(js_SV)\n",
    "                \n",
    "\n",
    "    print(\"--------------------indegree--------------------\")\n",
    "    print(JS_indegree)\n",
    "    print(sum(JS_indegree[0:10])/10)\n",
    "    print(sum(JS_indegree[10:20])/10)\n",
    "    print(sum(JS_indegree[20:30])/10)\n",
    "    print(sum(JS_indegree[30:40])/10)\n",
    "    print(\"--------------------outdegree--------------------\")\n",
    "    print(JS_outdegree)\n",
    "    print(sum(JS_outdegree[0:10])/10)\n",
    "    print(sum(JS_outdegree[10:20])/10)\n",
    "    print(sum(JS_outdegree[20:30])/10)\n",
    "    print(sum(JS_outdegree[30:40])/10)    \n",
    "    print(\"--------------------clustering coefficient--------------------\")\n",
    "    print(MAE_clustering)\n",
    "    print(sum(MAE_clustering[0:10])/10)\n",
    "    print(sum(MAE_clustering[10:20])/10)\n",
    "    print(sum(MAE_clustering[20:30])/10)\n",
    "    print(sum(MAE_clustering[30:40])/10)    \n",
    "    print(\"--------------------min distance--------------------\")\n",
    "    print(JS_mindistance)\n",
    "    print(sum(JS_mindistance[0:10])/10)\n",
    "    print(sum(JS_mindistance[10:20])/10)\n",
    "    print(sum(JS_mindistance[20:30])/10)\n",
    "    print(sum(JS_mindistance[30:40])/10)\n",
    "    print(\"--------------------weakly connected components--------------------\")\n",
    "    print(JS_wcc)\n",
    "    print(sum(JS_wcc[0:10])/10)\n",
    "    print(sum(JS_wcc[10:20])/10)\n",
    "    print(sum(JS_wcc[20:30])/10)\n",
    "    print(sum(JS_wcc[30:40])/10)    \n",
    "    print(\"--------------------strongly connected components--------------------\")\n",
    "    print(JS_scc)\n",
    "    print(sum(JS_scc[0:10])/10)\n",
    "    print(sum(JS_scc[10:20])/10)\n",
    "    print(sum(JS_scc[20:30])/10)\n",
    "    print(sum(JS_scc[30:40])/10)    \n",
    "    print(\"--------------------FLSV--------------------\")\n",
    "    print(JS_FLSV)\n",
    "    print(sum(JS_FLSV[0:10])/10)\n",
    "    print(sum(JS_FLSV[10:20])/10)\n",
    "    print(sum(JS_FLSV[20:30])/10)\n",
    "    print(sum(JS_FLSV[30:40])/10)\n",
    "    print(\"--------------------SV--------------------\")\n",
    "    print(JS_SV)\n",
    "    print(sum(JS_SV[0:10])/10)\n",
    "    print(sum(JS_SV[10:20])/10)\n",
    "    print(sum(JS_SV[20:30])/10)\n",
    "    print(sum(JS_SV[30:40])/10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions\n",
    "def plot_distribution(distributions, legends, xaxis, yaxis, filename, islog):\n",
    "    fig = plt.figure()\n",
    "    ax = fig . add_subplot (1,1,1)\n",
    "    ax.set_xlabel(xaxis)\n",
    "    ax.set_ylabel(yaxis)\n",
    "    if islog:\n",
    "        ax . set_xscale ('log')\n",
    "        ax . set_yscale ('log')\n",
    "    for distri in distributions:\n",
    "        ax . plot (distri.keys(), distri.values())\n",
    "    \n",
    "    ax.legend(legends, loc = 'best')\n",
    "    #fig . savefig (filename)\n",
    "    tikz_save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317080\n",
      "1049866\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''g = load_graph(\"Dataset/DBLP/com-dblp.ungraph.txt\", \"Undirected\")\n",
    "print(nx.number_of_nodes(g))\n",
    "print(nx.number_of_edges(g))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50180 99166\n"
     ]
    }
   ],
   "source": [
    "g = powerlaw_degree_graph_generator(56000, 2.3, seed = 1)\n",
    "print(nx.number_of_nodes(g), nx.number_of_edges(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 9.118407487869263 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#al, ab = build_attribute_list_and_bucket(\"Dataset/DBLP/com-dblp.all.cmty.txt\", \"communities\")\n",
    "start_time = time.time()\n",
    "al, ab = generate_attribute_list_and_bucket(g)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_adjlist(sub_g, \"RATS_\" + str(1) + \".adjlist.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist_to_edgelist(\"Dataset/DBLP/RATS/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.869808673858643 seconds ---\n",
      "5507\n",
      "--- 83.80635285377502 seconds ---\n",
      "65976\n",
      "--- 4.9952392578125 seconds ---\n",
      "19429\n",
      "--- 0.05984163284301758 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "sub_g_rats, it = random_walk_attribute_sampling(g, 0.1, al, ab, 0.001, seed = 1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(it)\n",
    "\n",
    "start_time = time.time()\n",
    "sub_g_mhrw, it = MH_random_walk_sampling(g, 0.1, seed = 1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(it)\n",
    "\n",
    "start_time = time.time()\n",
    "sub_g_rw, it = random_walk_with_restart_sampling(g, 0.1, 0.15, seed = 1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(it)\n",
    "\n",
    "start_time = time.time()\n",
    "sub_g_rn = random_node_sampling(g, 0.1, seed = 1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04691926807777591 0.1724558486863561 0.15549480712393837 0.4042771298701031\n"
     ]
    }
   ],
   "source": [
    "degree_full = degree_distribution(g)\n",
    "js_degree_rats = JS_divergence(degree_full, degree_distribution(sub_g_rats))\n",
    "js_degree_mhrw = JS_divergence(degree_full, degree_distribution(sub_g_mhrw))\n",
    "js_degree_rw = JS_divergence(degree_full, degree_distribution(sub_g_rw))\n",
    "js_degree_rn = JS_divergence(degree_full, degree_distribution(sub_g_rn))\n",
    "print(js_degree_rats, js_degree_rw, js_degree_mhrw, js_degree_rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Please add the following lines to your LaTeX preamble:\n",
      "\n",
      "\\usepackage[utf8]{inputenc}\n",
      "\\usepackage{fontspec} % This line only for XeLaTeX and LuaLaTeX\n",
      "\\usepackage{pgfplots}\n",
      "=========================================================\n",
      "Horizontal alignment will be ignored as no 'x tick label text width' has been passed in the 'extra' parameter\n",
      "Horizontal alignment will be ignored as no 'y tick label text width' has been passed in the 'extra' parameter\n"
     ]
    }
   ],
   "source": [
    "plot_distribution([degree_full, degree_distribution(sub_g_rats), degree_distribution(sub_g_rw), degree_distribution(sub_g_mhrw), degree_distribution(sub_g_rn)], [\"full\",\"RATS\",\"RWR\",\"MHRW\",\"RN\"], \"degree\", \"distribution\", \"degree.tikz\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_full = clustering_coefficient_distribution(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21172978376712018 0.13176390689847942 0.1589862001637943 0.15291736550996116\n"
     ]
    }
   ],
   "source": [
    "absolute_error_clustering_coefficient_rats = average_absolute_distance(clustering_full, clustering_coefficient_distribution(sub_g_rats))\n",
    "absolute_error_clustering_coefficient_mhrw = average_absolute_distance(clustering_full, clustering_coefficient_distribution(sub_g_mhrw))\n",
    "absolute_error_clustering_coefficient_rw = average_absolute_distance(clustering_full, clustering_coefficient_distribution(sub_g_rw))\n",
    "absolute_error_clustering_coefficient_rn = average_absolute_distance(clustering_full, clustering_coefficient_distribution(sub_g_rn))\n",
    "print(absolute_error_clustering_coefficient_rats,absolute_error_clustering_coefficient_mhrw,absolute_error_clustering_coefficient_rw,absolute_error_clustering_coefficient_rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Please add the following lines to your LaTeX preamble:\n",
      "\n",
      "\\usepackage[utf8]{inputenc}\n",
      "\\usepackage{fontspec} % This line only for XeLaTeX and LuaLaTeX\n",
      "\\usepackage{pgfplots}\n",
      "=========================================================\n",
      "Horizontal alignment will be ignored as no 'x tick label text width' has been passed in the 'extra' parameter\n",
      "Horizontal alignment will be ignored as no 'y tick label text width' has been passed in the 'extra' parameter\n"
     ]
    }
   ],
   "source": [
    "plot_distribution([clustering_full, clustering_coefficient_distribution(sub_g_rats), clustering_coefficient_distribution(sub_g_rw), clustering_coefficient_distribution(sub_g_mhrw),  clustering_coefficient_distribution(sub_g_rn)], [\"full\",\"RATS\",\"RWR\",\"MHRW\",\"RN\"], \"Degree\", \"Clustering Coefficient\", \"Power25_Clustering.tikz\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0003005459183324684 0.0 0.3765244262326446\n"
     ]
    }
   ],
   "source": [
    "cc_full = cc_distribution(g)\n",
    "js_cc_rats = JS_divergence(cc_full, cc_distribution(sub_g_rats))\n",
    "js_cc_mhrw = JS_divergence(cc_full, cc_distribution(sub_g_mhrw))\n",
    "js_cc_rw = JS_divergence(cc_full, cc_distribution(sub_g_rw))\n",
    "js_cc_rn = JS_divergence(cc_full, cc_distribution(sub_g_rn))\n",
    "print(js_cc_rats, js_cc_mhrw, js_cc_rw, js_cc_rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_edgelist(g, \"power2_3.edgelist\")\n",
    "nx.write_edgelist(sub_g_rats, \"power2_3_rats.edgelist\")\n",
    "nx.write_edgelist(sub_g_mhrw, \"power2_3_mhrw.edgelist\")\n",
    "nx.write_edgelist(sub_g_rw, \"power2_3_rw.edgelist\")\n",
    "nx.write_edgelist(sub_g_rn, \"power2_3_rn.edgelist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13030241435170442 0.020555129280529272 0.12994992042756603 0.06855161571839874\n"
     ]
    }
   ],
   "source": [
    "min_distance_distribution_g = read_plothop_distribution(\"hop.full.tab\")\n",
    "js_min_distance_rats = JS_divergence(min_distance_distribution_g, read_plothop_distribution(\"hop.rats.tab\"))\n",
    "js_min_distance_mhrw = JS_divergence(min_distance_distribution_g, read_plothop_distribution(\"hop.mhrw.tab\"))\n",
    "js_min_distance_rw = JS_divergence(min_distance_distribution_g, read_plothop_distribution(\"hop.rw.tab\"))\n",
    "js_min_distance_rn = JS_divergence(min_distance_distribution_g, read_plothop_distribution(\"hop.rn.tab\"))\n",
    "print(js_min_distance_rats, js_min_distance_mhrw, js_min_distance_rw, js_min_distance_rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_rats = clustering_coefficient_distribution(sub_g_rats)\n",
    "degree_rw = clustering_coefficient_distribution(sub_g_rw)\n",
    "degree_mhrw = clustering_coefficient_distribution(sub_g_mhrw)\n",
    "degree_rn = clustering_coefficient_distribution(sub_g_rn)\n",
    "degree_full = load_distribution(\"ijcai/degree_distribution_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Please add the following lines to your LaTeX preamble:\n",
      "\n",
      "\\usepackage[utf8]{inputenc}\n",
      "\\usepackage{fontspec} % This line only for XeLaTeX and LuaLaTeX\n",
      "\\usepackage{pgfplots}\n",
      "=========================================================\n",
      "Horizontal alignment will be ignored as no 'x tick label text width' has been passed in the 'extra' parameter\n",
      "Horizontal alignment will be ignored as no 'y tick label text width' has been passed in the 'extra' parameter\n"
     ]
    }
   ],
   "source": [
    "c_rats = clustering_coefficient_distribution(sub_g_rats)\n",
    "c_rw = clustering_coefficient_distribution(sub_g_rw)\n",
    "c_mhrw = clustering_coefficient_distribution(sub_g_mhrw)\n",
    "c_rn = clustering_coefficient_distribution(sub_g_rn)\n",
    "c_full = load_distribution(\"ijcai/clustering_coefficient_distribution_full\")\n",
    "plot_distribution([c_rats, c_rw, c_mhrw, c_rn, c_full], [\"RATS\",\"RWR\",\"MHRW\",\"RN\",\"full\"], \"degree\", \"clustering coefficient\", \"clustering.tikz\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Please add the following lines to your LaTeX preamble:\n",
      "\n",
      "\\usepackage[utf8]{inputenc}\n",
      "\\usepackage{fontspec} % This line only for XeLaTeX and LuaLaTeX\n",
      "\\usepackage{pgfplots}\n",
      "=========================================================\n",
      "Horizontal alignment will be ignored as no 'x tick label text width' has been passed in the 'extra' parameter\n",
      "Horizontal alignment will be ignored as no 'y tick label text width' has been passed in the 'extra' parameter\n"
     ]
    }
   ],
   "source": [
    "plot_distribution([degree_rats, degree_rw, degree_mhrw, degree_rn, degree_full], [\"RATS\",\"RWR\",\"MHRW\",\"RN\",\"full\"], \"degree\", \"distribution\", \"degree.tikz\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_evaluations_ijcai(subgraph, path_ff, subgraph_name):\n",
    "    # load distributions\n",
    "    degree_distribution_g = load_distribution(path_ff + \"degree_distribution_full\")\n",
    "    clustering_coefficient_distribution_g = load_distribution(path_ff + \"clustering_coefficient_distribution_full\")\n",
    "    cc_distribution_g = load_distribution(path_ff + \"connected_components_distribution_full\")\n",
    "    min_distance_distribution_g = read_plothop_distribution(path_ff + \"hop.tab\")\n",
    "    FLSV_g = load_distribution(path_ff + \"FLSV_full\")\n",
    "    SV_g = load_distribution(path_ff + \"SV_full\")\n",
    "    \n",
    "    degree_distribution_subg = degree_distribution(subgraph)\n",
    "    save_distribution(degree_distribution_subg, \"degree_distribution_\" + subgraph_name)\n",
    "    js_degree_distribution = JS_divergence(degree_distribution_g, degree_distribution_subg)\n",
    "    print(\"---------------degree---------------\")\n",
    "    print(js_degree_distribution)\n",
    "        \n",
    "    clustering_coefficient_distribution_subg = clustering_coefficient_distribution(subgraph)\n",
    "    save_distribution(clustering_coefficient_distribution_subg, \"clustering_coefficient_distribution_\" + subgraph_name)\n",
    "    absolute_error_clustering_coefficient = average_absolute_distance(clustering_coefficient_distribution_g, clustering_coefficient_distribution_subg)\n",
    "    print(\"---------------clustering coefficient---------------\")\n",
    "    print(absolute_error_clustering_coefficient)\n",
    "        \n",
    "    min_distance_distribution_subg = read_plothop_distribution(\"hop.\" + subgraph_name + \".tab\")\n",
    "    js_min_distance_distribution = JS_divergence(min_distance_distribution_g, min_distance_distribution_subg)\n",
    "    print(\"---------------min_distance---------------\")\n",
    "    print(js_min_distance_distribution)\n",
    "        \n",
    "    cc_distribution_subg = cc_distribution(subgraph)\n",
    "    save_distribution(cc_distribution_subg, \"connected_components_distribution_\" + subgraph_name)\n",
    "    js_cc_distribution = JS_divergence(cc_distribution_g, cc_distribution_subg)\n",
    "    print(\"---------------cc---------------\")\n",
    "    print(js_cc_distribution)\n",
    "    \n",
    "    FLSV_subg, SV_subg = get_SVD_distribution(subgraph, 100)\n",
    "    #save_distribution(FLSV_subg, \"FLSV_\" + subgraph_name)\n",
    "    save_distribution(SV_subg, \"SV_\" + subgraph_name)\n",
    "    #js_FLSV = JSD(sample_FLSV_distribution(FLSV_g, 0.1), FLSV_subg) # here JS_divergence gives bad normalization\n",
    "    js_SV = JS_divergence(SV_g, SV_subg)\n",
    "    #print(\"---------------FLSV---------------\")\n",
    "    #print(js_FLSV)\n",
    "    print(\"---------------SV---------------\")\n",
    "    print(js_SV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_evaluations_ijcai_di(subgraph, path_ff, subgraph_name):\n",
    "    # load distributions\n",
    "    indegree_distribution_g = load_distribution(path_ff + \"indegree_distribution_full\")\n",
    "    outdegree_distribution_g = load_distribution(path_ff + \"outdegree_distribution_full\")\n",
    "    clustering_coefficient_distribution_g = load_distribution(path_ff + \"clustering_coefficient_distribution_full\")\n",
    "    wcc_distribution_g = load_distribution(path_ff + \"weakly_connected_components_distribution_full\")\n",
    "    min_distance_distribution_g = read_plothop_distribution(path_ff + \"hop.tab\")\n",
    "    #FLSV_g = load_distribution(path_ff + \"FLSV_full\")\n",
    "    SV_g = load_distribution(path_ff + \"SV_full\")\n",
    "    \n",
    "    indegree_distribution_subg = indegree_distribution(subgraph)\n",
    "    save_distribution(indegree_distribution_subg, \"indegree_distribution_\" + subgraph_name)\n",
    "    js_indegree_distribution = JS_divergence(indegree_distribution_g, indegree_distribution_subg)\n",
    "    print(\"---------------indegree---------------\")\n",
    "    print(js_indegree_distribution)\n",
    "    \n",
    "    outdegree_distribution_subg = outdegree_distribution(subgraph)\n",
    "    save_distribution(outdegree_distribution_subg, \"outdegree_distribution_\" + subgraph_name)\n",
    "    js_outdegree_distribution = JS_divergence(outdegree_distribution_g, outdegree_distribution_subg)\n",
    "    print(\"---------------outdegree---------------\")\n",
    "    print(js_outdegree_distribution)\n",
    "        \n",
    "    clustering_coefficient_distribution_subg = clustering_coefficient_distribution(subgraph.to_undirected())\n",
    "    save_distribution(clustering_coefficient_distribution_g, \"clustering_coefficient_distribution_\" + subgraph_name)\n",
    "    absolute_error_clustering_coefficient = average_absolute_distance(clustering_coefficient_distribution_g, clustering_coefficient_distribution_subg)\n",
    "    print(\"---------------clustering coefficient---------------\")\n",
    "    print(absolute_error_clustering_coefficient)\n",
    "        \n",
    "    min_distance_distribution_subg = read_plothop_distribution(\"hop.\" + subgraph_name + \".tab\")\n",
    "    js_min_distance_distribution = JS_divergence(min_distance_distribution_g, min_distance_distribution_subg)\n",
    "    print(\"---------------min_distance---------------\")\n",
    "    print(js_min_distance_distribution)\n",
    "        \n",
    "    wcc_distribution_subg = wcc_distribution(subgraph)\n",
    "    save_distribution(wcc_distribution_subg, \"weakly_connected_components_distribution_\" + subgraph_name)\n",
    "    js_wcc_distribution = JS_divergence(wcc_distribution_g, wcc_distribution_subg)\n",
    "    print(\"---------------wcc---------------\")\n",
    "    print(js_wcc_distribution)\n",
    "        \n",
    "    FLSV_subg, SV_subg = get_SVD_distribution(subgraph, 100)\n",
    "    #save_distribution(FLSV_subg, \"FLSV_\" + subgraph_name)\n",
    "    save_distribution(SV_subg, \"SV_\" + subgraph_name)\n",
    "    #js_FLSV = JSD(sample_FLSV_distribution(FLSV_g, 0.1), FLSV_subg) # here JS_divergence gives bad normalization\n",
    "    js_SV = JS_divergence(SV_g, SV_subg)\n",
    "    #print(\"---------------FLSV---------------\")\n",
    "    #print(js_FLSV)\n",
    "    print(\"---------------SV---------------\")\n",
    "    print(js_SV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------degree---------------\n",
      "0.009170601663867295\n",
      "---------------clustering coefficient---------------\n",
      "0.2015752716473219\n",
      "---------------min_distance---------------\n",
      "0.07242657554049492\n",
      "---------------cc---------------\n",
      "0.0\n",
      "--- 5.759595632553101 seconds ---\n",
      "---------------SV---------------\n",
      "0.0009436313087025638\n"
     ]
    }
   ],
   "source": [
    "do_all_evaluations_ijcai(sub_g, \"Dataset/DBLP/results/\", \"RATS_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# repeat experiments 10 times with random seed 1 to 10\n",
    "def rwa_10():\n",
    "    spendtime = []\n",
    "    iterations = []\n",
    "    for i in range(1, 11):\n",
    "        rdm.seed(i)\n",
    "        start_time = time.time()\n",
    "        sample_g_rwa, it_rwa = random_walk_attribute_sampling(g, 0.1, al, ab, 0.01)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        spendtime.append(time.time() - start_time)\n",
    "        iterations.append(it_rwa)        \n",
    "        print(nx.number_of_nodes(sample_g_rwa))\n",
    "        print(nx.number_of_edges(sample_g_rwa))\n",
    "        \n",
    "        nx.write_adjlist(sample_g_rwa, \"rwa_\" + str(i) + \".adjlist.gz\")\n",
    "        #print(it_rwa)\n",
    "    print(spendtime)\n",
    "    print(sum(spendtime)/10)\n",
    "    print(iterations)\n",
    "    print(sum(iterations)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.933488607406616 seconds ---\n",
      "33677\n",
      "69855\n",
      "--- 7.266780376434326 seconds ---\n",
      "33510\n",
      "69606\n",
      "--- 8.126790046691895 seconds ---\n",
      "33574\n",
      "73775\n",
      "--- 4.513562917709351 seconds ---\n",
      "33576\n",
      "69966\n",
      "--- 4.594079256057739 seconds ---\n",
      "33682\n",
      "74159\n",
      "--- 4.17477011680603 seconds ---\n",
      "33594\n",
      "71678\n",
      "--- 5.804047584533691 seconds ---\n",
      "33648\n",
      "73586\n",
      "--- 5.85080885887146 seconds ---\n",
      "33660\n",
      "79870\n",
      "--- 7.525046110153198 seconds ---\n",
      "33497\n",
      "82159\n",
      "--- 6.0101237297058105 seconds ---\n",
      "33595\n",
      "69309\n",
      "[4.933488607406616, 7.266780376434326, 8.126790046691895, 4.513562917709351, 4.594079256057739, 4.17477011680603, 5.804047584533691, 5.85080885887146, 7.525046110153198, 6.0101237297058105]\n",
      "5.879949760437012\n",
      "[764, 420, 723, 250, 513, 607, 238, 466, 320, 283]\n",
      "458.4\n"
     ]
    }
   ],
   "source": [
    "rwa_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat experiments 10 times with random seed 1 to 10\n",
    "def ff_10():\n",
    "    spendtime = []\n",
    "    iterations = []\n",
    "    for i in range(1, 11):\n",
    "        start_time = time.time()\n",
    "        sample_g_rwa, it_rwa = forest_fire_sampling(g, 0.1, 0.15, seed = i)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        spendtime.append(time.time() - start_time)\n",
    "        iterations.append(it_rwa)        \n",
    "        print(nx.number_of_nodes(sample_g_rwa))\n",
    "        print(nx.number_of_edges(sample_g_rwa))\n",
    "        \n",
    "        nx.write_adjlist(sample_g_rwa, \"ff_\" + str(i) + \".adjlist.gz\")\n",
    "        #print(it_rwa)\n",
    "    print(spendtime)\n",
    "    print(sum(spendtime)/10)\n",
    "    print(iterations)\n",
    "    print(sum(iterations)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat experiments 10 times with random seed 1 to 10\n",
    "def MHRW_10():\n",
    "    spendtime = []\n",
    "    iterations = []\n",
    "    for i in range(1, 11):\n",
    "        start_time = time.time()\n",
    "        sample_g_MHRW, it_MHRW = MH_random_walk_sampling(g, 0.1, seed = i)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        spendtime.append(time.time() - start_time)\n",
    "        iterations.append(it_MHRW)        \n",
    "        print(nx.number_of_nodes(sample_g_MHRW))\n",
    "        print(nx.number_of_edges(sample_g_MHRW))\n",
    "        \n",
    "        nx.write_adjlist(sample_g_MHRW, \"MHRW_\" + str(i) + \".adjlist.gz\")\n",
    "        #print(it_rwa)\n",
    "    print(spendtime)\n",
    "    print(sum(spendtime)/10)\n",
    "    print(iterations)\n",
    "    print(sum(iterations)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10.709856033325195 seconds ---\n",
      "33486\n",
      "70274\n",
      "--- 10.3797607421875 seconds ---\n",
      "33486\n",
      "69569\n",
      "--- 10.211945295333862 seconds ---\n",
      "33486\n",
      "70029\n",
      "--- 10.36857008934021 seconds ---\n",
      "33486\n",
      "70010\n",
      "--- 11.006011009216309 seconds ---\n",
      "33486\n",
      "70489\n",
      "--- 10.53023886680603 seconds ---\n",
      "33486\n",
      "71581\n",
      "--- 10.257216453552246 seconds ---\n",
      "33486\n",
      "69767\n",
      "--- 9.935564756393433 seconds ---\n",
      "33486\n",
      "69581\n",
      "--- 10.161395072937012 seconds ---\n",
      "33486\n",
      "70687\n",
      "--- 10.537580728530884 seconds ---\n",
      "33486\n",
      "70962\n",
      "[10.709856033325195, 10.3797607421875, 10.211945295333862, 10.36857008934021, 11.006011009216309, 10.53023886680603, 10.257216453552246, 9.935564756393433, 10.161395072937012, 10.537580728530884]\n",
      "10.409813904762268\n",
      "[150979, 151188, 150061, 150817, 155102, 153258, 150793, 148563, 150518, 154549]\n",
      "151582.8\n"
     ]
    }
   ],
   "source": [
    "MHRW_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat experiments 10 times with random seed 1 to 10\n",
    "def ff_10_di():\n",
    "    spendtime = []\n",
    "    iterations = []\n",
    "    for i in range(1, 11):\n",
    "        start_time = time.time()\n",
    "        sample_g_rwa, it_rwa = directed_forest_fire_sampling(g, 0.1, 0.15, 0.3, seed = i)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        spendtime.append(time.time() - start_time)\n",
    "        iterations.append(it_rwa)        \n",
    "        print(nx.number_of_nodes(sample_g_rwa))\n",
    "        print(nx.number_of_edges(sample_g_rwa))\n",
    "        \n",
    "        nx.write_adjlist(sample_g_rwa, \"ff_\" + str(i) + \".adjlist.gz\")\n",
    "        #print(it_rwa)\n",
    "    print(spendtime)\n",
    "    print(sum(spendtime)/10)\n",
    "    print(iterations)\n",
    "    print(sum(iterations)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7.622659921646118 seconds ---\n",
      "377476\n",
      "2194729\n",
      "--- 7.462141990661621 seconds ---\n",
      "377476\n",
      "2177245\n",
      "--- 7.607639789581299 seconds ---\n",
      "377476\n",
      "2166383\n",
      "--- 7.234398126602173 seconds ---\n",
      "377476\n",
      "2058027\n",
      "--- 7.343960285186768 seconds ---\n",
      "377476\n",
      "2201303\n",
      "--- 7.3477888107299805 seconds ---\n",
      "377476\n",
      "2299502\n",
      "--- 7.453097343444824 seconds ---\n",
      "377476\n",
      "2205385\n",
      "--- 7.437936782836914 seconds ---\n",
      "377476\n",
      "2092968\n",
      "--- 7.557390928268433 seconds ---\n",
      "377476\n",
      "1926158\n",
      "--- 7.386552095413208 seconds ---\n",
      "377476\n",
      "2257447\n",
      "[7.622659921646118, 7.462141990661621, 7.607639789581299, 7.234398126602173, 7.343960285186768, 7.3477888107299805, 7.453097343444824, 7.437936782836914, 7.557390928268433, 7.386552095413208]\n",
      "7.445356607437134\n",
      "[377476, 377476, 377477, 377476, 377476, 377476, 377476, 377476, 377476, 377476]\n",
      "377476.1\n"
     ]
    }
   ],
   "source": [
    "ff_10_di()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_evaluations_jiayu_Di(path_ff):\n",
    "    # load distributions\n",
    "    indegree_distribution_g = load_distribution(path_ff + \"indegree_distribution_full\")\n",
    "    outdegree_distribution_g = load_distribution(path_ff + \"outdegree_distribution_full\")\n",
    "    clustering_coefficient_distribution_g = load_distribution(path_ff + \"clustering_coefficient_distribution_full\")\n",
    "    #scc_distribution_g = load_distribution(path_ff + \"strongly_connected_components_distribution_full\")\n",
    "    wcc_distribution_g = load_distribution(path_ff + \"weakly_connected_components_distribution_full\")\n",
    "     \n",
    "    # get all files from path_subgraphs\n",
    "    list_subgraphs = listdir(path_ff + \"subgraphs/\")\n",
    "    \n",
    "    # JS divergence for different methods\n",
    "    JS_indegree = []\n",
    "    JS_outdegree = []\n",
    "    MAE_clustering = []\n",
    "    #JS_scc = []\n",
    "    JS_wcc = []\n",
    "    \n",
    "    for sg in list_subgraphs:\n",
    "        print(sg)\n",
    "        subgraph = nx.read_adjlist(path_ff + \"subgraphs/\" + sg, create_using=nx.DiGraph())\n",
    "        subgraph_name = re.search('(.+?)\\.adjlist\\.gz', sg).group(1)\n",
    "        \n",
    "        indegree_distribution_subg = indegree_distribution(subgraph)\n",
    "        save_distribution(indegree_distribution_subg, \"indegree_distribution_\" + subgraph_name)\n",
    "        js_indegree_distribution = JS_divergence(indegree_distribution_g, indegree_distribution_subg)\n",
    "        JS_indegree.append(js_indegree_distribution)\n",
    "        \n",
    "        outdegree_distribution_subg = outdegree_distribution(subgraph)\n",
    "        save_distribution(outdegree_distribution_subg, \"outdegree_distribution_\" + subgraph_name)\n",
    "        js_outdegree_distribution = JS_divergence(outdegree_distribution_g, outdegree_distribution_subg)\n",
    "        JS_outdegree.append(js_outdegree_distribution)\n",
    "        \n",
    "        clustering_coefficient_distribution_subg = clustering_coefficient_distribution(subgraph.to_undirected())\n",
    "        save_distribution(clustering_coefficient_distribution_subg, \"clustering_coefficient_distribution_\" + subgraph_name)\n",
    "        absolute_error_clustering_coefficient = average_absolute_distance(clustering_coefficient_distribution_g, clustering_coefficient_distribution_subg)\n",
    "        MAE_clustering.append(absolute_error_clustering_coefficient)\n",
    "        '''\n",
    "        scc_distribution_subg = scc_distribution(subgraph)\n",
    "        save_distribution(scc_distribution_subg, \"Strongly_connected_components_distribution_\" + subgraph_name)\n",
    "        js_scc_distribution = JS_divergence(scc_distribution_g, scc_distribution_subg)\n",
    "        JS_scc.append(js_scc_distribution)\n",
    "        '''\n",
    "        wcc_distribution_subg = wcc_distribution(subgraph)\n",
    "        save_distribution(wcc_distribution_subg, \"weakly_connected_components_distribution_\" + subgraph_name)\n",
    "        js_wcc_distribution = JS_divergence(wcc_distribution_g, wcc_distribution_subg)\n",
    "        JS_wcc.append(js_wcc_distribution)\n",
    "        \n",
    "    print(\"--------------------indegree--------------------\")\n",
    "    print(JS_indegree)\n",
    "    print(sum(JS_indegree[0:1])/1)\n",
    "    print(\"--------------------outdegree--------------------\")\n",
    "    print(JS_outdegree)\n",
    "    print(sum(JS_outdegree[0:1])/1)\n",
    "    print(\"--------------------clustering coefficient--------------------\")\n",
    "    print(MAE_clustering)\n",
    "    print(sum(MAE_clustering[0:1])/1)\n",
    "    '''\n",
    "    print(\"--------------------s connected components--------------------\")\n",
    "    print(JS_scc)\n",
    "    print(sum(JS_scc[0:1])/1)\n",
    "    '''\n",
    "    print(\"--------------------w connected components--------------------\")\n",
    "    print(JS_wcc)\n",
    "    print(sum(JS_wcc[0:1])/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff_1.adjlist.gz\n",
      "--------------------indegree--------------------\n",
      "[0.03693899391092437]\n",
      "0.03693899391092437\n",
      "--------------------outdegree--------------------\n",
      "[0.08054901011870186]\n",
      "0.08054901011870186\n",
      "--------------------clustering coefficient--------------------\n",
      "[0.01940639302448519]\n",
      "0.01940639302448519\n",
      "--------------------w connected components--------------------\n",
      "[0.0014122529798967012]\n",
      "0.0014122529798967012\n"
     ]
    }
   ],
   "source": [
    "do_all_evaluations_jiayu_Di(\"../WWW19 Random Walk Attribute Sampling/Dataset/cit-Patents/ffresult/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_evaluations_jiayu(path_ff):\n",
    "    # load distributions\n",
    "    degree_distribution_g = load_distribution(path_ff + \"degree_distribution_full\")\n",
    "    clustering_coefficient_distribution_g = load_distribution(path_ff + \"clustering_coefficient_distribution_full\")\n",
    "    cc_distribution_g = load_distribution(path_ff + \"connected_components_distribution_full\")\n",
    "     \n",
    "    # get all files from path_subgraphs\n",
    "    list_subgraphs = listdir(path_ff + \"subgraphs/\")\n",
    "    \n",
    "    # JS divergence for different methods\n",
    "    JS_degree = []\n",
    "    MAE_clustering = []\n",
    "    JS_cc = []\n",
    "    \n",
    "    for sg in list_subgraphs:\n",
    "        print(sg)\n",
    "        subgraph = nx.read_adjlist(path_ff + \"subgraphs/\" + sg)\n",
    "        subgraph_name = re.search('(.+?)\\.adjlist\\.gz', sg).group(1)\n",
    "        \n",
    "        degree_distribution_subg = degree_distribution(subgraph)\n",
    "        save_distribution(degree_distribution_subg, \"degree_distribution_\" + subgraph_name)\n",
    "        js_degree_distribution = JS_divergence(degree_distribution_g, degree_distribution_subg)\n",
    "        JS_degree.append(js_degree_distribution)\n",
    "        \n",
    "        clustering_coefficient_distribution_subg = clustering_coefficient_distribution(subgraph)\n",
    "        save_distribution(clustering_coefficient_distribution_subg, \"clustering_coefficient_distribution_\" + subgraph_name)\n",
    "        absolute_error_clustering_coefficient = average_absolute_distance(clustering_coefficient_distribution_g, clustering_coefficient_distribution_subg)\n",
    "        MAE_clustering.append(absolute_error_clustering_coefficient)\n",
    "        \n",
    "        cc_distribution_subg = cc_distribution(subgraph)\n",
    "        save_distribution(cc_distribution_subg, \"connected_components_distribution_\" + subgraph_name)\n",
    "        js_cc_distribution = JS_divergence(cc_distribution_g, cc_distribution_subg)\n",
    "        JS_cc.append(js_cc_distribution)\n",
    "        \n",
    "    print(\"--------------------degree--------------------\")\n",
    "    print(JS_degree)\n",
    "    print(sum(JS_degree[0:10])/10)\n",
    "    print(\"--------------------clustering coefficient--------------------\")\n",
    "    print(MAE_clustering)\n",
    "    print(sum(MAE_clustering[0:10])/10)\n",
    "    print(\"--------------------connected components--------------------\")\n",
    "    print(JS_cc)\n",
    "    print(sum(JS_cc[0:10])/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff_1.adjlist.gz\n",
      "ff_10.adjlist.gz\n",
      "ff_2.adjlist.gz\n",
      "ff_3.adjlist.gz\n",
      "ff_4.adjlist.gz\n",
      "ff_5.adjlist.gz\n",
      "ff_6.adjlist.gz\n",
      "ff_7.adjlist.gz\n",
      "ff_8.adjlist.gz\n",
      "ff_9.adjlist.gz\n",
      "--------------------degree--------------------\n",
      "[0.025554809221114638, 0.026140535274700838, 0.025519772706450894, 0.03126645290484564, 0.027986437563074062, 0.023895187210118962, 0.022763609959794096, 0.024923236858656495, 0.027514082344845647, 0.02472520718722926]\n",
      "0.026028933123083055\n",
      "--------------------clustering coefficient--------------------\n",
      "[0.1394002903888917, 0.1572611412287203, 0.15824907576821215, 0.1734308632963491, 0.15726442520181555, 0.13769409063003074, 0.15164522196857033, 0.13968883066969442, 0.16616436255700245, 0.14773172101086027]\n",
      "0.1528530022720147\n",
      "--------------------connected components--------------------\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0001103910279615049, 1.5769070503103522e-05, 1.5769070503103522e-05, 0.0, 0.0, 1.5769070503103522e-05]\n",
      "1.5769823947081547e-05\n"
     ]
    }
   ],
   "source": [
    "do_all_evaluations_jiayu(\"../WWW19 Random Walk Attribute Sampling/Dataset/DBLP/ffresult/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_degree_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/degree_distribution_full\")\n",
    "ds_degree_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/DS_Amazon_distribution/DS_degree_distribution\")\n",
    "ff_degree_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/FF_Amazon_distribution/FF_degree_distribution\")\n",
    "rn_degree_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/RN_Amazon_distribution/RN_degree_distribution\")\n",
    "rwr_degree_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/RWR_Amazon_distribution/RWR_degree_distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Please add the following lines to your LaTeX preamble:\n",
      "\n",
      "\\usepackage[utf8]{inputenc}\n",
      "\\usepackage{fontspec} % This line only for XeLaTeX and LuaLaTeX\n",
      "\\usepackage{pgfplots}\n",
      "=========================================================\n",
      "Horizontal alignment will be ignored as no 'x tick label text width' has been passed in the 'extra' parameter\n",
      "Horizontal alignment will be ignored as no 'y tick label text width' has been passed in the 'extra' parameter\n"
     ]
    }
   ],
   "source": [
    "plot_distribution([full_degree_distribution, ds_degree_distribution, ff_degree_distribution, rn_degree_distribution, rwr_degree_distribution], [\"full\",\"DS\",\"FF\",\"RN\",\"RWR\"], \"degree\", \"distribution\", \"degree.tikz\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_clustering_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/clustering_coefficient_distribution_full\")\n",
    "ds_clustering_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/DS_Amazon_distribution/DS_clustering_coefficient_distribution\")\n",
    "ff_clustering_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/FF_Amazon_distribution/FF_clustering_coefficient_distribution\")\n",
    "rn_clustering_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/RN_Amazon_distribution/RN_clustering_coefficient_distribution\")\n",
    "rwr_clustering_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/RWR_Amazon_distribution/RWR_clustering_coefficient_distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Please add the following lines to your LaTeX preamble:\n",
      "\n",
      "\\usepackage[utf8]{inputenc}\n",
      "\\usepackage{fontspec} % This line only for XeLaTeX and LuaLaTeX\n",
      "\\usepackage{pgfplots}\n",
      "=========================================================\n",
      "Horizontal alignment will be ignored as no 'x tick label text width' has been passed in the 'extra' parameter\n",
      "Horizontal alignment will be ignored as no 'y tick label text width' has been passed in the 'extra' parameter\n"
     ]
    }
   ],
   "source": [
    "plot_distribution([full_clustering_distribution, ds_clustering_distribution, ff_clustering_distribution, rn_clustering_distribution, rwr_clustering_distribution], [\"full\",\"DS\",\"FF\",\"RN\",\"RWR\"], \"degree\", \"clustering coefficient\", \"clustering_coefficient.tikz\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cc_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/connected_components_distribution_full\")\n",
    "ds_cc_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/DS_Amazon_distribution/DS_cc_distribution_subg\")\n",
    "ff_cc_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/FF_Amazon_distribution/FF_cc_distribution_subg\")\n",
    "rn_cc_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/RN_Amazon_distribution/RN_cc_distribution_subg\")\n",
    "rwr_cc_distribution = load_distribution(\"../WWW19 Random Walk Attribute Sampling/distribution/RWR_Amazon_distribution/RWR_cc_distribution_subg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Please add the following lines to your LaTeX preamble:\n",
      "\n",
      "\\usepackage[utf8]{inputenc}\n",
      "\\usepackage{fontspec} % This line only for XeLaTeX and LuaLaTeX\n",
      "\\usepackage{pgfplots}\n",
      "=========================================================\n",
      "Horizontal alignment will be ignored as no 'x tick label text width' has been passed in the 'extra' parameter\n",
      "Horizontal alignment will be ignored as no 'y tick label text width' has been passed in the 'extra' parameter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\haotian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\axes\\_base.py:3152: UserWarning: Attempting to set identical left==right results\n",
      "in singular transformations; automatically expanding.\n",
      "left=1.0, right=1.0\n",
      "  'left=%s, right=%s') % (left, right))\n",
      "c:\\users\\haotian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\axes\\_base.py:3471: UserWarning: Attempting to set identical bottom==top results\n",
      "in singular transformations; automatically expanding.\n",
      "bottom=1.0, top=1.0\n",
      "  'bottom=%s, top=%s') % (bottom, top))\n"
     ]
    }
   ],
   "source": [
    "plot_distribution([full_cc_distribution, ds_cc_distribution, ff_cc_distribution, rn_cc_distribution, rwr_cc_distribution], [\"full\",\"DS\",\"FF\",\"RN\",\"RWR\"], \"connected components\", \"distribution\", \"plot_cc.tikz\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgsgsg, ti = MH_random_walk_sampling(g, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399796\n",
      "2275915\n"
     ]
    }
   ],
   "source": [
    "print(nx.number_of_nodes(sgsgsg))\n",
    "print(nx.number_of_edges(sgsgsg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_distribution_g1 = degree_distribution(g)\n",
    "degree_distribution_subg1 = degree_distribution(sgsgsg)\n",
    "js_degree_distribution = JS_divergence(degree_distribution_g1, degree_distribution_subg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0667109228141074\n"
     ]
    }
   ],
   "source": [
    "print(js_degree_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 21.443650245666504 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# 1st -- real attributes, 2nd -- fake attributes\n",
    "start_time = time.time()\n",
    "al, ab = build_attribute_list_and_bucket(\"Dataset/LiveJournal/com-lj.all.cmty.txt\", \"communities\")\n",
    "#al, ab = generate_attribute_list_and_bucket(g)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facebook-100 use the 2nd one\n",
    "\n",
    "# assortativity(g, al)\n",
    "# assortativity(sample_g, fitst_attribute(al))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008669206277570207\n"
     ]
    }
   ],
   "source": [
    "# caculate alpha\n",
    "alpha = calculate_alpha(g, al, ab, 0.001)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rw_10():\n",
    "    spendtime = []\n",
    "    iterations = []\n",
    "    for i in range(1, 11):\n",
    "        rdm.seed(i)\n",
    "        start_time = time.time()\n",
    "        sample_g_rw, it_rw = random_walk_sampling(g, 0.1)\n",
    "        spendtime.append(time.time() - start_time)\n",
    "        iterations.append(it_rw)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(nx.number_of_nodes(sample_g_rw))\n",
    "        print(nx.number_of_edges(sample_g_rw))\n",
    "        \n",
    "        nx.write_adjlist(sample_g_rw, \"rw_\" + str(i) + \".adjlist.gz\")\n",
    "        #print(it_rw)\n",
    "    print(spendtime)\n",
    "    print(sum(spendtime)/10)\n",
    "    print(iterations)\n",
    "    print(sum(iterations)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ra_10():\n",
    "    spendtime = []\n",
    "    for i in range(1, 11):\n",
    "        rdm.seed(i)\n",
    "        start_time = time.time()\n",
    "        sample_g_ra = random_attribute_sampling(g, 0.1, al, ab, alpha)\n",
    "        spendtime.append(time.time() - start_time)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(nx.number_of_nodes(sample_g_ra))\n",
    "        print(nx.number_of_edges(sample_g_ra))\n",
    "        \n",
    "        nx.write_adjlist(sample_g_ra, \"ra_\" + str(i) + \".adjlist.gz\")\n",
    "    print(spendtime)\n",
    "    print(sum(spendtime)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rn_10():\n",
    "    spendtime = []\n",
    "    for i in range(1, 11):\n",
    "        rdm.seed(i)\n",
    "        start_time = time.time()\n",
    "        sample_g_rn = random_node_sampling(g, 0.1)\n",
    "        spendtime.append(time.time() - start_time)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(nx.number_of_nodes(sample_g_rn))\n",
    "        print(nx.number_of_edges(sample_g_rn))\n",
    "        \n",
    "        nx.write_adjlist(sample_g_rn, \"rn_\" + str(i) + \".adjlist.gz\")\n",
    "    print(spendtime)\n",
    "    print(sum(spendtime)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.541752815246582 seconds ---\n",
      "46502\n",
      "36448\n",
      "--- 2.1248695850372314 seconds ---\n",
      "46502\n",
      "37647\n",
      "--- 3.1224608421325684 seconds ---\n",
      "46502\n",
      "44762\n",
      "--- 6.355346918106079 seconds ---\n",
      "46502\n",
      "26850\n",
      "--- 1.1389167308807373 seconds ---\n",
      "46502\n",
      "59557\n",
      "--- 2.535579204559326 seconds ---\n",
      "46502\n",
      "38807\n",
      "--- 5.062677383422852 seconds ---\n",
      "46502\n",
      "33684\n",
      "--- 2.8125081062316895 seconds ---\n",
      "46502\n",
      "29029\n",
      "--- 4.97372031211853 seconds ---\n",
      "46502\n",
      "27590\n",
      "--- 4.5154969692230225 seconds ---\n",
      "46502\n",
      "29270\n",
      "[3.541752815246582, 2.1248695850372314, 3.1224608421325684, 6.355346918106079, 1.1389167308807373, 2.535579204559326, 5.062677383422852, 2.8125081062316895, 4.97372031211853, 4.5154969692230225]\n",
      "3.6183328866958617\n",
      "[4454, 2461, 3198, 7195, 359, 3003, 6557, 4147, 6689, 5931]\n",
      "4399.4\n"
     ]
    }
   ],
   "source": [
    "rwa_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 324.8993136882782 seconds ---\n",
      "46502\n",
      "47410\n",
      "--- 330.45741415023804 seconds ---\n",
      "46502\n",
      "44729\n",
      "--- 348.1521461009979 seconds ---\n",
      "46502\n",
      "45149\n",
      "--- 377.1786787509918 seconds ---\n",
      "46502\n",
      "37780\n",
      "--- 374.17223167419434 seconds ---\n",
      "46502\n",
      "41555\n",
      "--- 364.2274127006531 seconds ---\n",
      "46502\n",
      "38972\n",
      "--- 367.2516658306122 seconds ---\n",
      "46502\n",
      "41233\n",
      "--- 347.682918548584 seconds ---\n",
      "46502\n",
      "40846\n",
      "--- 356.6663920879364 seconds ---\n",
      "46502\n",
      "41815\n",
      "--- 344.10869097709656 seconds ---\n",
      "46502\n",
      "43644\n",
      "[324.8993136882782, 330.45741415023804, 348.1521461009979, 377.1786787509918, 374.17223167419434, 364.2274127006531, 367.2516658306122, 347.682918548584, 356.6663920879364, 344.10869097709656]\n",
      "353.47968645095824\n",
      "[576302, 576544, 600876, 644909, 642280, 627545, 631301, 608446, 621528, 616971]\n",
      "614670.2\n"
     ]
    }
   ],
   "source": [
    "rw_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.8638954162597656 seconds ---\n",
      "46502\n",
      "28506\n",
      "--- 0.8627023696899414 seconds ---\n",
      "46502\n",
      "29771\n",
      "--- 0.8870625495910645 seconds ---\n",
      "46502\n",
      "27404\n",
      "--- 0.8714683055877686 seconds ---\n",
      "46502\n",
      "29409\n",
      "--- 0.9036834239959717 seconds ---\n",
      "46502\n",
      "35774\n",
      "--- 0.8751156330108643 seconds ---\n",
      "46502\n",
      "31876\n",
      "--- 0.8482997417449951 seconds ---\n",
      "46502\n",
      "29301\n",
      "--- 0.8594601154327393 seconds ---\n",
      "46502\n",
      "28435\n",
      "--- 0.8858973979949951 seconds ---\n",
      "46502\n",
      "29923\n",
      "--- 0.849846363067627 seconds ---\n",
      "46502\n",
      "25537\n",
      "[0.8638954162597656, 0.8627023696899414, 0.8870625495910645, 0.8714683055877686, 0.9036834239959717, 0.8751156330108643, 0.8482997417449951, 0.8594601154327393, 0.8858973979949951, 0.849846363067627]\n",
      "0.8707431316375732\n"
     ]
    }
   ],
   "source": [
    "ra_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0624842643737793 seconds ---\n",
      "46502\n",
      "8471\n",
      "--- 0.0624847412109375 seconds ---\n",
      "46502\n",
      "9265\n",
      "--- 0.07810640335083008 seconds ---\n",
      "46502\n",
      "9141\n",
      "--- 0.07810592651367188 seconds ---\n",
      "46502\n",
      "9290\n",
      "--- 0.07285475730895996 seconds ---\n",
      "46502\n",
      "8321\n",
      "--- 0.07810664176940918 seconds ---\n",
      "46502\n",
      "8228\n",
      "--- 0.06246137619018555 seconds ---\n",
      "46502\n",
      "7615\n",
      "--- 0.0781090259552002 seconds ---\n",
      "46502\n",
      "8659\n",
      "--- 0.0780785083770752 seconds ---\n",
      "46502\n",
      "8419\n",
      "--- 0.07812666893005371 seconds ---\n",
      "46502\n",
      "9147\n",
      "[0.0624842643737793, 0.0624847412109375, 0.07810640335083008, 0.07810592651367188, 0.07285475730895996, 0.07810664176940918, 0.06246137619018555, 0.0781090259552002, 0.0780785083770752, 0.07812666893005371]\n",
      "0.07289183139801025\n"
     ]
    }
   ],
   "source": [
    "rn_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist_to_edgelist(\"Dataset/_nonattri_FourSquare/subgraphs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ra_1.adjlist.gz\n",
      "ra_10.adjlist.gz\n",
      "ra_2.adjlist.gz\n",
      "ra_3.adjlist.gz\n",
      "ra_4.adjlist.gz\n",
      "ra_5.adjlist.gz\n",
      "ra_6.adjlist.gz\n",
      "ra_7.adjlist.gz\n",
      "ra_8.adjlist.gz\n",
      "ra_9.adjlist.gz\n",
      "rn_1.adjlist.gz\n",
      "rn_10.adjlist.gz\n",
      "rn_2.adjlist.gz\n",
      "rn_3.adjlist.gz\n",
      "rn_4.adjlist.gz\n",
      "rn_5.adjlist.gz\n",
      "rn_6.adjlist.gz\n",
      "rn_7.adjlist.gz\n",
      "rn_8.adjlist.gz\n",
      "rn_9.adjlist.gz\n",
      "rwa_1.adjlist.gz\n",
      "rwa_10.adjlist.gz\n",
      "rwa_2.adjlist.gz\n",
      "rwa_3.adjlist.gz\n",
      "rwa_4.adjlist.gz\n",
      "rwa_5.adjlist.gz\n",
      "rwa_6.adjlist.gz\n",
      "rwa_7.adjlist.gz\n",
      "rwa_8.adjlist.gz\n",
      "rwa_9.adjlist.gz\n",
      "rw_1.adjlist.gz\n",
      "rw_10.adjlist.gz\n",
      "rw_2.adjlist.gz\n",
      "rw_3.adjlist.gz\n",
      "rw_4.adjlist.gz\n",
      "rw_5.adjlist.gz\n",
      "rw_6.adjlist.gz\n",
      "rw_7.adjlist.gz\n",
      "rw_8.adjlist.gz\n",
      "rw_9.adjlist.gz\n",
      "--------------------degree--------------------\n",
      "[0.45818194786778665, 0.4664760753905184, 0.46923931581319644, 0.46952129758774275, 0.46698066758723433, 0.4762869433372674, 0.4733060583372337, 0.47206912745432383, 0.47493022959312237, 0.4868098754852803, 0.5294999521963408, 0.5252770044149981, 0.5249873796623326, 0.548438490941737, 0.5446443149274129, 0.5279912534457556, 0.5488761027954654, 0.5355248039239315, 0.5273376317774692, 0.5318385627135238, 0.44679628076737066, 0.4416304961360158, 0.4348361171991053, 0.44157672261210257, 0.3761862698740184, 0.4607908034387549, 0.35222896907307044, 0.39141980307647994, 0.3739664382051542, 0.42959210556528893, 0.25177198939369383, 0.2490546767554136, 0.25688763852305563, 0.25126781418113886, 0.2496045139344769, 0.23825736757467464, 0.2476430508544496, 0.2501860274119174, 0.25109967127756416, 0.24268395317654612]\n",
      "0.4713801538453707\n",
      "0.5344415496798968\n",
      "0.41490240059473604\n",
      "0.24884567030829308\n",
      "--------------------clustering coefficient--------------------\n",
      "[0.12846729677414342, 0.13088274378877057, 0.2143480771773377, 0.1917901538656362, 0.22452523177864273, 0.21481710325665299, 0.20524640952648238, 0.1816760517741264, 0.2283577731034938, 0.23437646089772546, 0.18424996919907333, 0.15272871997557533, 0.21565389489289258, 0.25015949081219774, 0.2473870465922147, 0.199580031117148, 0.24371098382240397, 0.23036819528814742, 0.21126299790169456, 0.18996932783526405, 0.28790782558736544, 0.26446299828228825, 0.2816052016814717, 0.28423126184894587, 0.25105482971382537, 0.30312932753714456, 0.2090965283261959, 0.18625118876698157, 0.20839779529514854, 0.2702373968044987, 0.16048707029696932, 0.15217830809509592, 0.16098788984928383, 0.15438410714785306, 0.16133247348846297, 0.16067659714314353, 0.15686002675374824, 0.1495092609913281, 0.1614559581177499, 0.16146581776545377]\n",
      "0.19544873019430115\n",
      "0.21250706574366118\n",
      "0.2546374353843866\n",
      "0.15793375096490886\n",
      "--------------------min distance--------------------\n",
      "[0.11609424683795377, 0.12316563947729953, 0.0952878193655955, 0.10230209853670669, 0.1615484127775284, 0.09296585923999134, 0.09703351481556766, 0.09471690775448405, 0.08686845830724987, 0.1042705925820302, 0.13386323932134925, 0.149328400414388, 0.20804482534444801, 0.13887106869614785, 0.16269949619711332, 0.12519076992096645, 0.14643287671331318, 0.1154361207334782, 0.13157130835550968, 0.12806374977381574, 0.18672092050011901, 0.19089512170481604, 0.1533239924415355, 0.15617764406183587, 0.15488732109605485, 0.1653699781951432, 0.17947824801387857, 0.1342825752299479, 0.17391379437088395, 0.1657082145490132, 0.35754572598019285, 0.36616287142687876, 0.38222200103543047, 0.3559656575397454, 0.3797601682063805, 0.35716452769283125, 0.37375496122163154, 0.33977539986586947, 0.35422725562591806, 0.3570304915352511]\n",
      "0.1074253549694407\n",
      "0.14395018554705297\n",
      "0.1660757810163228\n",
      "0.36236090601301296\n",
      "--------------------connected components--------------------\n",
      "[0.4230429902120978, 0.43270026556716346, 0.44582395359116767, 0.440866264238708, 0.4275628960530673, 0.45553313543686524, 0.45001925645819085, 0.441322242893496, 0.4548800556663801, 0.47333735537623767, 0.5041660429797155, 0.49210478611380193, 0.4832995294748228, 0.5715122660358709, 0.5652356857324339, 0.5060562492672886, 0.571928108476695, 0.5309422206839631, 0.5052960764954797, 0.5043177150361835, 0.3477178116345421, 0.3363663664948007, 0.34512572214203097, 0.3536347083247877, 0.2874052952108004, 0.3699378698958835, 0.25232537583265335, 0.31489079837943645, 0.2826868846020427, 0.33926004790852193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0.4445088415493374\n",
      "0.5234858680296255\n",
      "0.32293508804255\n",
      "0.0\n",
      "--------------------FLSV--------------------\n",
      "[0.0470468298521728, 0.033641142783053496, 0.04720466642332112, 0.05626710201862732, 0.060725527374156894, 0.04360672549730357, 0.05663375013271675, 0.044966073661779814, 0.053528777961329946, 0.043421796859763453, 0.04455320461933113, 0.05778383466194914, 0.05887970588458759, 0.08529113267299637, 0.07270874507372173, 0.0723029497855924, 0.052385893669866034, 0.0638789877913382, 0.05924719488555587, 0.07070075244250917, 0.027848439610572442, 0.028949055056886067, 0.02711979369417901, 0.03194601486070493, 0.02556018564509545, 0.032907535414437535, 0.0353625118129902, 0.026749458943339723, 0.03185252685020913, 0.036135639791895165, 0.043575643701189756, 0.04845740750045624, 0.05946093620243606, 0.054976498155618156, 0.07746290801694936, 0.07335933246600883, 0.07502287372693688, 0.06767805175393457, 0.03611174761531866, 0.05195377230519714]\n",
      "0.04870423925642252\n",
      "0.06377324014874476\n",
      "0.030443116168030963\n",
      "0.05880591714440457\n",
      "--------------------SV--------------------\n",
      "[0.11334497132026833, 0.11223667069958321, 0.07332198920186528, 0.08985584651954515, 0.08259085725433923, 0.07314107819231008, 0.07835796269036699, 0.10072665140917003, 0.0690257591175456, 0.056020005744848334, 0.09642535380260941, 0.12492074229157346, 0.10304497582320238, 0.030340125158772935, 0.02966513490648648, 0.08653249491149939, 0.03494307087826787, 0.05922391625956802, 0.08764585816918569, 0.1132353415289975, 0.005128825576990792, 0.0022044101936797844, 0.006382003616681509, 0.0037917871511883305, 0.00888240160669973, 0.010580455158875068, 0.007056056608452543, 0.0004504779879939136, 0.004864171262402905, 0.013954072942873985, 0.0020367098939173545, 0.0023253440270298853, 0.0017959577052408804, 0.002180912300807414, 0.0020296512982174164, 0.00241792070988911, 0.0020316398014017523, 0.002637993228923996, 0.001991341872336072, 0.0021827010653172607]\n",
      "0.08486217921498422\n",
      "0.07659770137301632\n",
      "0.0063294662105838565\n",
      "0.002163017190308114\n"
     ]
    }
   ],
   "source": [
    "do_all_evaluations_udg(g, \"Dataset/_nonattri_FourSquare/subgraphs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
